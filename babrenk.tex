\documentclass[german,version-2020-11]{uzl-thesis}


% Copy this file as a template for your thesis. You will have to take
% action at all places marked by
%
% !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
% !!! Your action is needed here !!!
% !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
%
% The first place your action is needed is the first line of this
% document:
%
%
% Language of the thesis:
%
% You must use either 'german' or 'english' above, depending on the
% language used in the main text. This will automatically setup a lot
% of things in the background.
%
%
% Version of the class:
%
% You must specify which version of the thesis class is to be
% used. This is important in case the class style changes in later
% years, but we still want an older thesis to look the same, even when
% things are changed in the class.
%
% Do not change or remove the version-xxxx key.
%
%
% Text encoding:
%
% Your thesis *must* be encoded in utf8 (unicode), which is the
% default in most editors these days. Do *not* change this to latin8.



%%%
%
% Main setup:
%
%%%
%
% You must use the \UzLThesisSetup command to specify numerous things
% about your thesis. This includes the entries on the title page, the 
% abstracts, and the bibliography style. You do so by specifying
% so-called "values" for so-called "keys". For instance, 
% for the key "Autor" you must provide your name as the value. You do
% so by writing 'Autor = {Max Mustermann}', that is, the value is put
% into curly braces. You can use the \UzLThesisSetup command
% repeatedly and the order in which you provide the keys is not
% important. 
%
% Everything shown on the title page must be in German -- even
% if the thesis is written in English! Just insert German text for
% German keys and English text for English keys (like 'Abstract' needs
% English text, while 'Zusammenfassung' needs German text).

\UzLThesisSetup{
  %
  % !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  % !!! Your action is needed here !!!
  % !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  %
  % First, specify the institut or clinic at which the thesis was
  % written. You get the logo file from them (make sure it has the
  % correct size, namely the same as the example). If they do not have
  % a logo, the university's default logo is used.
  %
  % The 'verfasst' gets two arguments. Change the first to {an der}
  % for clinics, as in 'Verfasst = {an der}{Medizinischen Klinik I}'
  %
  Logo-Dateiname        = {uzl-thesis-logo-itcs.pdf},
  Verfasst              = {am}{Institut für Informationssysteme},
  %
  % The titles:
  %
  Titel auf Deutsch     = {
    Topic-Modellierung für die Zuordnung von Kundenanfragen zu Sachbearbeitern
  }, 
  Titel auf Englisch    = {
    Topic Modeling ... 
  },
  %
  % Author and supervisor:
  % 
  % Note that the 'Betreuer' or 'Betreuerin' is the supervisor, that
  % is, the professor who officially supervises the thesis. If there
  % is also an assistent of the professor who helped (typically a
  % lot), use 'Mit Unterstützung von' to thank that person. If the
  % thesis was mainly written 'externally' at some company or another
  % institute, point this out using 'Weitere Unterstützung'. 
  % 
  % For your own name, do *not* add things like "BSc" or "BSc
  % cand.". For the supervisor, you should normally include
  % "Prof. Dr." or "PD Dr." (ask your supervisor, what is
  % appropriate), but nothing more (so no
  % "Univ.-Prof. Dr. Dr. h.c. mult." unless your supervisor insists).  
  %
  Autor                 = {Leonard Brenk},
  Betreuerin            = {Prof. Dr. Ralf Möller},
  % 
  % Optional: Supporting persons and institutions. The text should be
  % in German, even for an English thesis.
  %
  Mit Unterstützung von = {Dr. Jinghua Groppe, Felix Kuhr, Magnus Bender},
  % 
  %   Weitere Unterstützung = {
  %     Die Arbeit ist im Rahmen einer Tätigkeit bei der Firma Muster GmbH
  %     entstanden.
  %   },
  %
  %
  % Your Degree Programm (Studiengang)
  %
  % Specify 'Bachelorarbeit' or 'Masterarbeit' and the degree
  % programme. Make sure the name of programme is correct and not
  % some abbreviation or some incorrect variant. For instance:
  % 'Medizinische Ingenierwissenschaft', but not 'MIW';
  % 'Medizinische Informatik', but not 'Medizin-Informatik';
  % 'Informatik', but not 'Informatik (SSE)'.
  %
  % Use German names for German programmes and English names for
  % English ones, so 'Infection Biology', not 'Infektionsbiologie'. 
  % For programmes that have a German bachelor and an English master,
  % use the German name for a bachelor thesis and the English name for
  % the master thesis.
  %
  Bachelorarbeit,
  Studiengang           = {Informatik},
  %
  % Date on which the thesis is turned in German, formatted the
  % traditional German way:
  %
  Datum                 = {1. Oktober 2021},
  %
  % The English abstract. You must always provide abstracts in German
  % and in English. 
  %
  Abstract              = {
    It is not easy to write a thesis that does not only advance
    science, but that is also a pleasure to read. While the scientific
    contribution of a thesis is undoubtedly of greater importance, the
    impact of \emph{writing well} should not be underestimated: If
    the person who grades a thesis finds no pleasure in the reading,
    that person are also unlikely to find pleasure in giving outstanding
    grades. A well-written text uses good German or English phrasing with a clear and correct 
    sentence structure and language rhythm, there are no spelling
    mistakes and the author's arguments are presented in a
    clear, logical and understandable manner using well-chosen
    examples and explanations. In addition, a nice-to-read font and a
    pleasing layout are also helpful. The \LaTeX\ class presented in
    this document helps with the latter: It contains a number of
    ready-to-use designs and 
    takes care of many small typographical chores.
  },
  Zusammenfassung       = {
    Es ist nicht leicht, eine Abschlussarbeit so zu schreiben, dass sie
    nicht nur inhaltlich gut ist, sondern es auch eine Freude ist, sie
    zu lesen. Diese Freude ist aber wichtig: Wenn die Person, die die 
    Arbeit benoten soll, wenig Gefallen am Lesen der Arbeit findet,
    so wird sie auch wenig Gefallen an einer guten Note
    finden. Glücklicherweise gibt es einige Kniffe, gut lesbare
    Arbeiten zu schreiben. Am wichtigsten ist zweifelsohne, dass
    die Arbeit in gutem Deutsch oder Englisch verfasst wurde mit klarem
    Satzbau und gutem Sprachrhythmus, dass keine Rechtschreib- oder
    Grammatikfehlern im Text auftauchen und dass die Argumente der
    Autorin oder des Autors klar, logisch, verständlich und gut
    veranschaulicht dargestellt werden. Daneben sind aber auch gut
    lesbare Schriftbilder und ein angenehmes Layout hilfreich. Die Nutzung
    dieser \LaTeX-Vorlage hilft der Schreiberin oder dem Schreiber
    dabei zumindest bei Letzterem: Sie umfasst gute, sofort nutzbare
    Designs und sie kümmert sich um viele typographische
    Details.  
  },
  %
  % Optional: 'Danksagungen' (German) or 'Acknowledgements'
  % (English). Both keys are optional and both have the same effect of
  % adding an acknowledgements text after the abstracts and before the
  % table of contents.
  %
  Acknowledgements      = {
    This is the place where you can thank people and institutions, do
    not try to do this on the title page. The only exception is in
    case you wrote your thesis while working or staying at a company or abroad. Then you
    should use the \Latex{Weitere Unterstützung} key to provide a text
    (in German) that acknowledges the company or foreign
    institute. For instance, you could use texts like »Die Arbeit
      ist im Rahmen einer Tätigkeit bei der Firma Muster GmbH
      entstanden« or »Die Arbeit ist im Rahmen eines
      Forschungsaufenthalts beim Institut für Dieses und Jenes an der
      Universität Entenhausen entstanden«. Do not name and thank
      individual persons from the company or foreign institute on the
      title page, do that here. 
  },
  % Bibliography style: Choose between
  % 
  % 'Alphabetische Bibliographie'
  % for all degree programmes in the natural sciences 
  % 
  % 'Numerische Bibliographie'
  % alternative for all other degree programmes
  % 
  % Either will load biblatex and setup the citation methods and the
  % bibliography styles correctly. You should not mess with them.
  % 
  Alphabetische Bibliographie,
  % Alternatively:
  % Numerische Bibliographie
}




%%%%%%%%%%%%%%%%%%%%
%
% Styling the thesis
%
%%%%%%%%%%%%%%%%%%%%
%
% Creating a visually pleasing layout and choosing fonts is not
% easy. Furthermore, different people have different preferences. Of
% course, for the University of Lübeck, the dean of studies could just
% force everyone to use one specific layout and font, but that seems a
% bit drastic and, also, it seems nice that thesis by different people
% have an individual style even though they all stick to the same
% overall structure.
%
% For these reasons, I (Till Tantau) have spend quite some time on
% designing a flexible layout and styling mechanism for theses.
%
% Basically, the overall structure of the thesis is fixed by the
% thesis class and so are many structural elements. For instance, you
% cannot change the order in which the abstract and table of contents
% are shown, you cannot move the bibliography elsewhere, indeed, the
% bibliography style is also fixed. Likewise, the text on the title
% page is fixed.
%
% Although many things are fixed, you *can* change several other
% things. For instance, you can change the font used for the main
% text, you can change which font is used for titles and headings or
% you can change whether titles and headlines are centered or flushed
% left.
%
% There are many LaTeX packages for changing such things. You are
% kindly asked *not to use them*. Rather, use (only) the options
% offered by the thesis class. All possible choices and combinations
% there have been tested by me and produce nice results; what happens
% with other packages no one knows and might no longer conform to what
% is expected by the university. As you will see, you still have a
% lot of options.
%
%
% Technical note: All styling is done via the command
%
% \UzLStyle{...}
%
% where ... is a key-value list just as for \UzLThesisSetup. The
% difference is just that everything having to do with styling as
% controlled by \UzLStyle, while the more “formal” setup keys are
% controlled by \UzLThesisSetup.
%
%%%
%
% Designs
%
%
% A \emph{design} is a whole set of font and layout options bundled
% together. They have been chosen in such a way that a visually
% pleasing “overall appearance” results.
%
%
% \UzLStyle{computer modern oldschool design}
%
% The look of this design mimics the “classical” way a paper or report
% created with \LaTeX\ looks like: The Computer Modern font is used,
% bold face fonts are used for headlines, only black and white are
% used as colors. This design reminds me of older scientific
% documents, especially from the computer science community where
% \LaTeX\ was used very early.
%
%
% \UzLStyle{computer modern basic design}
%
% A slightly less “oldschool” version of the previous design. It is
% still a classic design in the sense that it uses the Computer Modern
% font and that it still has this “good old \LaTeX” look, but some
% more modern aspects (like colors!) have been added.
%
% Note that this design uses Myriad for the title page (one of the
% “modern aspect”), which means that his font must be installed.
%
%
% \UzLStyle{computer modern scholary design}
%
% In my opinion, this is the ultimate “scholary design”: The thesis
% will look like it has been typeset by hand some 150 years ago and
% then printed by a university press. There is really nothing “modern”
% about it and the word in the name of the design is just part of the
% name of the “Computer Modern” font.
%
%
% \UzLStyle{pagella basic design}
%
% A, well, basic design that uses the Pagella font rather than the
% Computer Modern font. Especially the bold face version of this font
% looks nicer than the Computer Modern counterpart. Also, Pagella,
% while still having a “bookish” look, still feels a bit fresher than
% Computer Modern. 
%
%
% \UzLStyle{pagella centered design}
%
% A variant of the basic Pagella design that centers all
% headlines. A nice alternative to the basic version.
%
%
% \UzLStyle{pagella contrast design}
%
% This design tries to create some visual friction by contrasting the
% sans serif headline font (in bold!) with the main text. I find it a
% visually very interesting combination.
%
%
% \UzLStyle{alegrya basic design}
%
% The third variant of the basic design, this time using the Alegrya
% font. 
%
%
% \UzLStyle{alegrya scholary design}
%
% The Alegrya version of the previous “scholary” design. Unlike the
% Computer Modern version, this design does not look old, but more
% fresh -- while still creating the impression that the text must be
% about a very scientific subject. 
%
%
% \UzLStyle{alegrya stylish design}
%
% The design is quite similar to the scholary version for the Alegrya
% font, but with even more modern additions. “Stylish” is the word
% that comes to my mind.
%
%
\UzLStyle{alegrya modern design}
%
% A design that uses the sans serif version of the Alegrya font for
% the headlines. This is a nice modern overall design.
%
%%%




%%%%%%%%
%
% Now, include the package you need here using \usepackage. 
%
% However, many standard packages are already loaded by the class:
%
% amsmath, amssymb, amsthm, babel, biblatex, csquotes, etoolbox,
% filecontents, fontspec, geometry, hyperref, tikz (with libraries
% arrows.meta, positioning and shapes), varioref, url 
%
% Indeed, in many cases you will not need any extra packages.
%
%%%%%%%


\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{comment}
\usepackage{tikz}
\usepackage{amsmath}
\usetikzlibrary{arrows, automata, positioning}
\usepackage{pgfplots}
\usepackage{algorithm2e}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{shapes.geometric, arrows}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{caption}
\usepackage{float}


\begin{document}

%
% The title page and table of contents will be inserted automatically
% here. 
%


\chapter{Einleitung}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%EINLEITUNG
% In a German thesis write: \chapter{Einleitung}


% !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
% !!! Your action is needed here !!!
% !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
%
% Replace with your own introduction:

% DELETE the following line for your own thesis - it causes trouble!
\lstMakeShortInline[style=code,style=inline,language={[LaTeX]tex},moretexcs={chapter}]|



\section{Contributions of this Thesis}



\section{Related Work}
td-idf
word-embeddings

\section{Sturktur dieser Arbeit}
Der Bereich Data Science und Datenverarbeitung durchläuft aktuell eine starke Welle an Innovation und Veränderung. Deshalb wird in dieser Arbeit zuerst eine Einleitung die Motivation und das Prinzip der Topic-Modellierung beschreiben. Gefolgt wird die allgemeine Einleitung von der Motivation und Aufgabenstellung in Bezug auf das Anwendungsbeispiel des Zweckverbands Ostholstein (ZVO). Die Anwendung der Algorithmen wird im Laufe der Arbeit mit theoretischen Grundlagen erklärt, die eine klare Notation fordern: diese ist im Teil der Notationen gelistet. Das Prinzip der Topic-Modellierung kann mit unterschiedlichen Ansätzen implementiert werden. Beispiele sind: LDA, NNF und LSA. Da sich diese in ihrem Ansatz und in ihrer Umsetzung unterscheiden, werden im nächsten Abschnitt die bekanntesten dieser Modelle verglichen, indem ihre Vorgehens- und Funktionsweise untersucht werden. Die Methode, die in dieser Arbeit genauer beleuchtet wird, ist Latent Dirichlet Allocation. Der dritte Abschnitt wird von der Herkunft, dem Format und der Verarbeitung der Daten der ZVO handeln. Dabei wird der Anwendungsfall und die ZVO genauer erklärt und ein Einblick in die Daten, deren Reinigung und Verarbeitung gegeben. Darauf folgt das Konzept der Lösungsstrategie, um die Aussagekraft und Qualität der Daten für Topic-Modellierung erfolgreich zu untersuchen. Dafür wird der Ablauf des Algorithmus anschaulich dargestellt und die Prozesskette der Analyse im Detail aufgeschlüsselt. Im vierten Abschnitt findet die Implementierung des im dritten Abschnitt erklärten Konzepts statt. Für diese wird eine Methode der Topic-Modellierung ausgewählt und in einer vorher definierten und begründeten Programmierumgebung realisiert. Die Umsetzung wird anhand des gelisteten Algorithmus zeilenweise erklärt. Der Output ist in die Arbeit integriert, um dessen zentrale Rolle in der Datenanalyse zu unterstreichen und die Ergebnisse verständlich begründen zu können. Die detaillierte Analyse folgt im nächsten Abschnitt, der sich damit befasst, die Ergebnisse der Implementierung im Anwendungsfall der ZVO zu interpretieren. Dabei wird begründet, wie gut sich die Daten für eine Topic-Modellierung der ZVO Daten eignen und was die Ergebnisse über die Qualität der Daten aussagt. Die Arbeit wird abgeschlossen mit der Zusammenfassung der gesamten Analyse und einem Ausblick für die ZVO in Bezug auf Topic-Modellierung. Die gewonnenen Ergebnisse der Datenqualität sollen der ZVO nach der Ergebnisgewinnung zu einer höheren Effizienz in der Datenverarbeitung verhelfen und Aufschluss über Handlungsbedarf und Optimierungspotential in den Datensätzen geben.


\section{Motivation}
Die digitalisierte Welt generiert täglich riesige Mengen an neuen Informationen. Von E-Books, Blogs über Nachrichten-Websites und Magazinen bis hin zu mobilen Anwendungen auf dem Smartphone, immer mehr Menschen verlassen sich auf und richten ihr Leben nach dem Internet aus. Das Zeitalter des Big-Data ermöglicht es Nutzern unbegrenzt viele Daten zu generieren und zu sammeln. Die Kapazitäten, die ein Mensch aufbringen kann, um solche Massen an Daten zu organisieren und zu verstehen, sind schon lange übertroffen. Vor allem durch die steigende digitale Kommunikation und stetig sinkenden Speicherplatzkosten, erhöht sich die Menge an zu speichernden Einsen und Nullen. Laut Statista wurden 2018 33 Zettabyte an Daten generiert mit einer prognostizierten Steigerung bis 2025 um 530\% auf 175 Zettabyte. Dieser dramatische Anstieg zeigt die Dringlichkeit für effiziente Algorithmen und Modelle der Datenverarbeitung. Neben der reinen Handhabung solcher Daten steigt aber auch das Bewusstsein, aus diesen Daten Verständnis und Potentiale zu schaffen. Besonders Suchverfahren gewinnen an Bedeutung, wenn in großen, unübersichtlichen Datenmengen bestimmte Informationen gefragt sind. Für die Mehrheit bieten Firmen, wie Google, diese Anwendung an. Zwar kann durch die Keyword-basierten Suche das passende Dokument gefunden werden, jedoch schlägt die Suchmaschine fehl, wenn nach einer Menge von Dokumente mit einem übergreifenden Thema gefragt ist. Um mehrere Dokumente auf geteilte Themen zu untersuchen, wird die sogenannte Topic-Modellierung verwendet. Topic-Modellierung (dt. Themenmodellierung) beschreibt eine Gruppe von Verfahren, die es ermöglichen, große elektronische Datensammlungen automatisiert zu durchsuchen, organisieren und zu verstehen. Es können Muster innerhalb der Daten entdeckt und Topics extrahiert werden. Ein Topic stellt dabei eine Menge an Wörtern dar, häufig gemeinsam in einem Kontext vorkommen. Einem Topic über Hunde würden beispielsweise die Wörter Bellen, Fell und Halsband zu einer hohen Wahrscheinlichkeit angehören, da sie oft gemeinsam in dem übergeordneten Thema über Hunde gebraucht werden. Die Modellierung solcher Topics wird als Topic-Modellierung bezeichnet, da in einer Datenmenge nach mehreren solcher Topics gesucht wird. Diese können zu unterschiedlichen Anteilen in dem Dokument vertreten sein. Ein Dokument ist dabei ein Teil eines Korpus, also einer Menge mehrerer Dokumente. Dabei stellen Topic-Modelle statistische Modelle dar, die Verwendung in der Inferenz abstrakter Topics in unsortierten Datenmengen finden. Topic Modelling gehört zu dem Bereich des Natural Language Processing, also der Verarbeitung natürlicher Sprache. Es verbindet Computerlinguistik, Informatik und Künstliche Intelligenz, um die Potentiale der Sprachverarbeitung mit der heutigen Technik auszuschöpfen. In einer Welt von exponentiell wachsenden Datenmengen finden Methoden der Topic-Modellierung stetig eine breitere Anwendung. Bereits heute wird Topic-Modellierung in vielen Bereichen der Wirtschaft, Wissenschaft und Informationstechnologie verwendet. So findet die Topic-Modellierung unter anderem Anwendung bei Zusammenfassungen, Spam Filtern, Internet of Things (IOT), Healthcare, Blockchain, Chatbots, FAQs oder HR. Dies zeigt wie umfangreich das Anwendungsspektrum der Topic-Modellierung ist. Um semantische Folgerungen aus Datenmengen zu generieren, gibt es verschiedene Ansätze – in dieser Arbeit wird es um das generative Modell Latent Dirichlet Allocation(LDA) gehen. Dabei werden ähnliche Wörter, die in ähnlichen Kontexten vorkommen in Topics gruppiert. Die Grundlagen des LDA liegen bei der Verallgemeinerung des bereits $1999$ veröffentlichten Probabilistic Latent semantic Analysis (PLSA). Im Gegensatz zu anderen Machine Learning Methoden im Bereich der Datenverarbeitung, hat Topic-Modellierung die Besonderheit, dass ein Dokument nicht nur zu einem Topic zugeordnet werden kann, wie z.B. bei Clusteralgorithmen. Bei der Topic-Modellierung wird jedes Dokument durch eine Verteilung an Topics beschreiben, das bedeutet in jedem Dokument sind immer alle Topics zu finden - nur zu einem bestimmten Anteil. Genauso sind in einem Topic immer alle Worte zu einer bestimmten Wahrscheinlichkeit vorhanden. Bei einem Artikel, der zu $90\%$ über Sport und $10\%$ über Politik handelt, werden somit neun mal mehr Wörter bezüglich Sport zu finden sein, als über Politik. Topic-Modellierung wird den unüberwachten Lernmethoden des Data Minings zugeordnet, also der Extraktion von Muster und Trends in Datenmengen durch die Anwendung statistischer Algorithmen. Das bedeutet, dass die Topics ohne die Einwirkung von manuell erzeugten Labels gefunden werden. Im Lernprozess werden dann Verteilungen basierend auf den bislang vorgenommenen Zuordnungen iterativ angepasst und verbessert - jedoch alles ohne menschliches Zutun. 

\section{Ziel}
Diese Arbeit wird die Theorie des Topic-Modellierung anhand des Beispiels des Zweckverband Ostholstein (ZVO) in der Praxis implementieren und die bezüglichen Parameter im Sinne des ZVO bewerten. Der ZVO ist ein Unternehmen, das in Norddeutschland in der Energie-, Entwässerungs-, Internet- und Entsorgungsbranche tätig ist.  Es erhält jährlich ein große Menge an Kundenanfrage, die von Mitarbeitern gelesen und klassifiziert wird. Die klassifizierte Kundenanfrage wird dann an die zuständige Abteilung weitergeleitet. Dieser Vorgang ist sowohl zeitintensiv, als auch fehleranfällig und führt zu einer Ineffizienz in der Wertschöpfungskette. Der Prozess soll zukünftig automatisch durch einen Klassifikationsmechanismus funktionieren. \\
Diese Arbeit verfolgt zwei Ziele: 
\begin{enumerate}
\item Vorhersage der Qualität der Klassifikation durch Untersuchung der Datenqualität
\item Analyse der vordefinierten Abteilungen durch Topiczuordnung
\end{enumerate}

Diese Arbeit beschäftigt sich mit der Vorhersage der Qualität des Klassifikators, indem die Qualität der manuell erstellten Kategorien und Kundenanfrage-Gruppen untersucht und mit den Ergebnissen verschiedenen Topic Modellierungen verglichen wird. Das Ergebnis eines Topic Models hängt stark von der Qualität der Daten ab, die sie als Input bekommt. Diese Daten durchlaufen eine Reinigungsphase, bevor sie klassifiziert werden, um sie in eine gut zu verarbeitende Form zu bringen. Der Prozess der Reinigung kann Einfluss auf das Ergebnis haben. Ziel dieser Arbeit ist es, Erkenntnisse über die Qualität des Klassifikators zu treffen, in Abhängigkeit zu den verwendeten Daten. Somit wird durch die Nutzung von LDA-Modellen die Qualität der Daten untersucht und Prognosen über einer höheren Qualität der Klassfikiation anhand der Daten gemacht. 

\chapter{Grundlagen}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%GRUNDLADEN


\section{Notation}

\begin{itemize}
	\item Ein Wort wird mit $w \in N$ mit $N$ als Menge aller Worte bezeichnet
	\item Eine Menge Wörter ergeben ein Dokument $d \in D$ mit $D$ als Menge aller Dokumente
	\item Eine Menge an Dokumenten ergibt einen Korpus $D$
	\item Eine Menge an Wörtern ergibt eine Topic $z \in K$ mit $K$ als Menge aller Topics
	\item Ein Topic-Modell wird mit $M$ beschrieben
\end{itemize}


\section{Modellvergleich}

Das Feld des Information Retrieval bietet vielseitige Verfahren, um Topic-Modellierung zu betreiben. Bei vielen geht es um eine Dimensionsreduktion des Datensatzes, die Aufschluss und Erkenntnis über die Datenmenge geben soll. In dieser Arbeit wird die Latent Dirichlet Allocation(LDA) verwendet, das als Bindeglied zwischen statistichen und probabilistischen Maßen agiert. LDA wurde durch die Optimierung und Vermischung verschiednener vorangegangener Verfahren entwickelt. Die wichtigsten Verfahren in der Entwicklung von LDA sind im Folgenden aufgelistet und erklärt: 

\subsection{Term Frequency - Inverse Document Frequency(Tf-idf)}
Das Verfahren der (Tf-idf) beschreibt ein statistisches Maß, um die Wichtigkeit von Wörtern in einer Gruppe von Dokumenten zu beurteilen. Die Term-Frequency(Tf) erstellt eine Matrix, die die Anzahl der Wörter auf die Dokumente abbildet, in denen sie enthalten sind. Um zu vermeiden, dass Wörter, die wenig Aussagekraft für den Korpus haben, wie das Wort (und), bestimmt die Inverse Document Frequency(idf) wie spezifisch ein Wort für einen gesamten Korpus ist. Das einzelne Dokument ist dafür demnach nicht mehr wichtig. Die Idee dabei ist, dass Wörter, die sehr selten vorkommen ein Dokument unter Umständen stärker beeinflussen können, als ein Wort, das so häufig vorkommt, dass es schon keine Aussagekraft oder semantische Relevanz mehr hat. Das Verfahren verwendet keine probabilistischen Elemente sondern ist eine rein statistisch Möglichkeit herauszufinden, wie wichtig ein Wort eines Dokuments für das Gesamtthema des Dokuments ist. 

\subsection{Latent Semantic Analysis}
Viele Verfahren der Topic-Modellierung gehen auf das sogenannte Latent Semantic Analysis (LSA) zurück, das zuerst 1990 erwähnt wurde. LSA baut auf dem Prinzip der Tf-idf auf, indem es die Tf-Matrix nutzt und adaptiert. Bei dem Verfahren geht es um das Finden von sogenannten Hauptkomponenten in Dokumenten. Grob können Hauptkomponenten als allgemeine Überthemen verstanden werden. So wäre Politik die Hauptkomponente, die von Wörtern wie Politiker, Bundestag und Regierung beschrieben wird. Mathematisch gesehen verbindet man mit der Hauptkomponente die Hauptkomponentenanalyse, die als statistische Methode versucht, viele Variablen mithilfe von Eigenvektoren und Kovarianzen in einer vereinfachten Form mit einem möglichst geringen Informationsverlust darszustellen. Dabei ist das Ziel die Tf-Matrix möglichst verkleinert darzustellen, die die Anzahl der Vorkommen für jedes Wort pro Dokument abbildet. Sie wird in Matrizen aus Eigenvektoren und Eigenwerten aufgespalten, wodurch es möglich ist, die wichtigen Hauptkomponenten aus der meist sehr großen Tf-matrix zu extrahieren. Diese Matrix wird als Produkt von drei Matrizen dargestellt, von denen die mittlere eine Diagonalmatrix darstellt. Die Werte auf der Diagonalen lassen daraus die Topics der Textmenge ablesen. LSA stellt sich als ein attraktives Verfahren heraus, da es Synonyme besser erkennen kann, als LDA und wird heutzutage unter anderem intensiv in dem Bereich des Digital Marketings genutzt. 

\subsection{Probabilistic Latent Semantic Analysis}
Im Gegensatz zu LSA, was Singulärwertzerlegung für die Topic-Modellierung nutzt, verfolgt Probabilistic Latent Semantic Analysis (PLSA) einen anderen Ansatz. Die Kernidee ist, ein Wahrscheinlichkeitsmodell zu finden, das ein Dokument erzeugt, dass zu der Tf Matrix passt. PLSA hat als erstes Modell dem Konzept der Topic-Modellierung Wahrscheinlichkeiten hinzugefügt. Dabei sind Das Dokument und Wort bekannt, das assoziierte Topic jedoch nicht. Der grundlegende Gedanke ist, dass ein Dokument generiert werden kann, indem zuerst das Dokument mit einer bestimmten Wahrscheinlichkeit gewählt wird, dann ein Topic und aus diesem Topic dann ein Wort - alles mit Wahrscheinlichkeiten beschrieben. Es lässt sich der Fall, dass Dokument $d$ über Topic $z$ handelt, mit einer Wahrscheinlichkeit von $P(z \mid d)$ beschreiben. Dazu berechnet $P(w \mid z)$, wie wahrscheinlich es ist, dass das Wort $w$ aus einem gegebenen Dokument $d$ zu Topic $z$ gehört. Somit führt PLSA die Gesamtwahrscheinlich ein, dass ein Wort in einem gegebenen Dokument zu finden ist: $P(D,W) = P(D) \sum_{Z}{P(Z\mid D)P(W\mid Z)}$. Die Gleichung verbindet die Wahrscheinlichkeit ein bestimmtes Dokument zu erhalten mit der eines Wortes in diesem bedingt durch die Topic-Verteilung in diesem Dokument. 

\subsection{Latent Dirichlet Allocation}
Basierend auf LSA und PLSA ergibt sich eine der meist verbreiteten Verfahren, das Latent Dirichlet Allocation (LDA). Dieses Verfahren ist sehr ähnlich zu PLSA, das durch zwei Dirichlet-Verteilungen ergänzt wurde. LDA liegt, genau wie PLSA, ein generierender Prozess zugrunde, der jedes Dokument basierend auf mehreren Wahrscheinlichkeitsverteilungen generieren kann. Dabei wird zuerst das Dokment $d$ mit $P(d)$ gewählt, für dieses dann eine Topic $z$ mit $P(z \mid d)$ und anschließend die Wörter w mit $P(w \mid z)$. Grundsätzlich werden Dokumente bei LDA als die Dokument-Topic-Verteilung dargstellt, die die Ausprägungen verschiedener Topics in einem Dokument beschreibt. Für die Topics gibt es die Topic-Word-Verteilung, die die Wahrscheinlichkeit beschreibt, dass ein bestimmtes Wort in einer gewissen Regularität in einem Themenbereich vorkommt. Dabei geht man davon aus, dass ein Dokument eine Verteilung von Topics ist, während ein Topic als eine Verteilung über Wörter betrachtet wird. Der Unterschied zu PLSA liegt in zwei hinzugfügten Dirichlet Verteilungen, die die Zuordnung von Dokumenten zu Topics maßgeblich beeinflussen.
Die Wahrscheinlichkeit, dass ein bestimmtes Dokument generiert wird, ist das Produkt der Wahrscheinlichkeiten der Dokumnet-Topic-Verteilung und Topic-Wort-Verteilung mit den Wahrscheinlichkeiten der zwei hinzugefügten multinomialen Verteilungen. So werden erst die Topics zufällig ausgewählt, wie in der Dirichlet-Verteilung definiert, und aus diesen dann, mithilfe der zweiten Dirichlet-Verteilung, Wörter aus diesen Topics hergeleitet, wodurch das Enddokument entsteht. Die Herangehensweise ist in der Realität meistens umgekehrt, da selten Dokumente auf basis bekannter Topics generiert werden müssen, sondern eher Topics von bekannten Dokumenten gefunden werden sollen. 

%\subsection{Non-Negative Matrix Factorization}
%Ein weiteres Verfahren, das auch mit Matrizen funktioniert, wird „Non-Negative Matrix Factorization“ (NMF) genannt. Dabei wird eine Matrix, die Wörter auf Dokumente abbildet, in zwei Teilmatrizen faktorisiert. Dabei müssen alle Werte positiv sein. Faktorisierung bezeichnet die Darstellung einer Matrix durch die Multiplikation zweier anderer Matrizen. Dabei ist es bei NNM nicht immer möglich die originale Matrix mit der Multiplikation zu regenerieren, deshalb wird diese bestmöglich approximiert. 
%Die erste Teilmatrix stellt die Topics in Dokumenten, die zweite die Wörter in Topics dar. Dadurch kann Speicherplatz gespart, und Themen aufgedeckt werden. Das Verfahren beginnt mit zwei möglichen faktorisierten Matrizen und verbessert sich durch die Errorfunktion iterativ, bis das Ergebnis gut genug ist. Dabei werden die errechneten Werte mit der gegebenen Matrix verglichen und angepasst.


\section{Grundlagen der Latent Dirichlet Allocation }
Latent Dirichlet Allocation ist ein grundlegendes und bekanntes Verfahren aus der maschinellen Verarbeitung natürlichen Sprachen. Begründet wird dies unter anderem auf der Komplexität der damals bestehenden Techniken der Textverarbeitung. So waren Clusteralgorithmen zu starr in ihrem Anwendungsumfeld, während Dimensionsreduktionen, wie die Hauptkomponentenanalyse von LSA Ergebnisse lieferte, die sehr schwer zu interpretieren waren. Das Prinzip der Topic-Modellierung basiert auf einer Menge an Dokumenten, die den Korpus darstellen. Dabei werden bei LDA alle Dokumente als Menge von Wörtern angenommen. Diese sind als sogenannter Bag of Words modelliert. Das bedeutet, dass die Wörter als Menge ohne Reihenfolge interpretiert werden und nur die Häufigkeit der Begriffe in dieser Menge relevant ist. LDA ist ein generatives Modell, das durch unterschiedliche Einflussgrößen, wie in Abbildung \ref{fig:ldagraphic} dargestellt, Dokumente erstellen kann. \\

Bezüglich der Namensgebung, steht Latent für alles, was wir im Vorhinein nicht kennen. Im Fall LDA handelt es sich um die Themen, die in einem Dokument zu einem bestimmten Teil vertreten sind. Dirichlet beschreibt eine Verteilung von Verteilungen. Dies ist vergleichbar mit einem Würfel, bei dem regulierbar ist, wie gleichmäßig die Zahlen gewürfelt werden. Dabei ist der Würfel eine Verteilung und die Aufteilung der Gleichmäßigkeit auch. Beim Topic Modeling bedeutet Dirichlet eine Verteilung von Topics in Dokumenten und eine Verteilung von Wörtern in Topics. Die Dirichlet-Verteilungen sind die ausschlaggebende Veränderung im Vergleich zu PLSA. Durch sie kann die Stärke der Ausprägung der Topics in der Dokument-Topic-Verteilung und der Wörter in der Topic-Word-Verteilung bestimmt werden. Die Allocation weist mithilfe der errechneten Dirichlet-Verteilungen Topics Wörter und Dokumenten Topics zu. Eine Besonderheit bei der Topic-Modellierung mit LDA ist, dass die Anzahl der gesuchten Themen \mathcal{K} vorgegeben werden muss. Oft ist diese vorher jedoch nicht bekannt und muss über Hilfsverfahren, wie der Perplexitätsberechnung ermittelt werden. Dabei wird gemessen, wie geeignet die das Modell für die Verarbeitung der gegebenen Daten ist. Die Funktionsweise von LDA ist über folgende graphische Abbildung beschrieben: \\

\begin{figure}[h]
\centering
\begin{tikzpicture}
	\draw (0,0) [very thick] rectangle (9,3);
	\node at (8.5,0.5) {N};
	\node at (8.5,0.5) {N};
	\draw [very thick](6,3.5) rectangle (9,5.5);
	\node at (0.35,0.35) {D};
	\node at (0.35,0.35) {D};
	\draw [very thick](3,0.25) rectangle (8.75,2.75);
    \node at (8.75,3.75) {K};
    \node at (8.75,3.75) {K};    
	\draw (1.5,1.5) circle [ultra thick, radius = 0.8]  node {$\theta$};
	\draw (4.5,1.5)  circle [very thick, radius = 0.8] node {$\mathcal{Z}$};
	\draw (7.5,1.5) circle [ultra thick, radius = 0.8] node {$W$}; 
	\draw (-1.5,1.5) circle [very thick, radius = 0.8] node {$\alpha$};
	\draw (7.5,4.5) circle  [very thick, radius = 0.8] node {$\beta$};
	\draw (4.5,4.5) circle  [very thick, radius = 0.8] node {$\phi$};
	\draw [->,ultra thick] (5.3,4.5) -- (6.7,4.5);
	\draw [->,ultra thick] (7.5,3.7) -- (7.5,2.3);
	\draw [->,ultra thick] (5.3,1.5) -- (6.7,1.5);
	\draw [->,ultra thick] (2.3,1.5) -- (3.7,1.5);
	\draw [->,ultra thick] (-0.7,1.5) -- (0.7,1.5);
	
\end{tikzpicture}\caption{Graphische Darstellung von LDA}
\label{fig:ldagraphic}
\end{figure}

Dabei beschreibt $W$ als einzige nicht verborgene Variable eines von $N$ Wörtern des Dokuments. Das Wort ist semantisch einem Thema $Z$ zugeordnet. Das Thema wiederum hängt von der Themen-Verteilung $\theta$ des Dokuments ab, das als ein Element der $\mid M \mid$ vorliegenden Dokumente betrachtet wird. Bei LDA werden zwei Verteilungen aus den Dokumenten $d \in D$ und $k \in K$ gelernt: 

\begin{itemize}
\item die Dokument-Topic-Verteilung $\theta_d$, die angibt, mit welcher Wahrscheinlichkeit das Dokument zu jedem Themen gehört und 
\item die Topic-Wort-Verteilung $\phi_k$, die die Wahrscheinlichkeit berechnet, dass ein Wort einem Thema angehört. 
\end{itemize}\\

$M = LDA(D)$ beschreibt ein LDA Modell, das auf dem Korpus $D$ trainiert wurde. Das Modell und dessen Verteilungen kann durch die Parameter $\alpha$ und $\beta$ angepasst werden. $\alpha$ bestimmt die Intensität der Dokument-Themen-Verteilung, während $\beta$ die der Topic-Wort-Verteilung beeinflusst. Bei einem großen $\alpha$ ist die Verteilung der Topics in einem Dokument ähnlicher. Bei LDA werden zwei widersprüchliche Bedingungen verfolgt, die von den beiden Parametern beeinflusst werden können:
\begin{enumerate}
\item Erstens strebt man für alle Wort eines bestimmten Dokuments so wenig zugeordnete Themen an, wie möglich.
\item Zweitens soll ein Thema über so wenig relevante Wörter wie möglich verfügen.
\end{enumerate}\\

Die beiden Ziele stehen in einer Wechselbeziehung zueinander, da eine minimale Anzahl an vertretenen Topics in einem Dokument zu maximal vielen Wörtern in diesen Topics führt. Die minimale Anzahl an Topics wäre erreicht, wenn man alle Wörter eines Dokuments einem Thema zuweist. Dadurch verfügt das Topic jedoch über alle Wörter des Dokuments. $\alpha$ befindet sich in dem Bereicht $[0,1]$ mit sinnvollen Werten zwischen $[0.01, 0.1]$, während $\beta =0.01$ durchschnittlich die besten Ergebnisse liefert. Große Werte führen zu einer Gleichverteilung, die wiederum eine Verschlechterung der Perplexität bedeutet. Somit bietet die Perplexität ein Mittel, um $\alpha$ und $\beta$ optimal für die individuelle Anwendung zu finden. \\



\subsection{Der generative Prozess} 
Bei der Erstellung eines Klassifikators gibt es zwei unterschiedliche Herangehensweisen: den deskriptiven und den generativen Ansatz. Bei der deskriptiven, oder auch beschreibenden Statistik geht es um die sinnvolle und übersichtliche Darstellung empirischer Daten durch zum Beispiel Tabellen oder Kennzahlen. Betrachtet man zwei Variablen, ein bekanntes $X$ und eine gesuchte Variable $Y$, dann wird im deskriptiven Modell die bedingte Wahrscheinlichkeit $P(Y \mid X)$ betrachtet. Im Gegensatz dazu ist bei generativen Modellen die Wahrscheinlichkeit von $X$ und $Y$ gemeinsam relevant, also $P(X \cap Y)$ . Bei generativen Modellen besteht die Möglichkeit, Ausgabeinstanzen zu erstellen. Bei LDA handelt es sich um einen generativen Algorithmus. Das bedeutet nicht direkt, dass LDA bei der Ausführung aktiv neue Dokumente entwickelt, sondern stellt eine Erklärung dar, wie die Zusammenstellung der Topics und deren Wörter in einem spezifischen Fall entstanden sind. Es könnten theoretisch im Fall einer neuen Dokumentzuordnung zu Abteilungen für jede Abteilung zufällig Dokumente generiert werden. Diese würden zufällig gewählte Wörter basierend auf der Topic-Verteilung aneinanderreihen und mit hoher Wahrscheinlichkeit keinen Sinn ergeben. Trotzdem könnten diese Dokumente mit den den Dokumenten der Abteilungen verglichen und auf Ähnlichkeit werden. Dann könnten die Parameter solange angepasst werden, bis das Ergebnis akzeptabel ist. Der reguläre Weg ist jedoch andersherum: Wir suchen nicht das Dokument zu einer gegebenen Dokument-Topic-Verteilung und einer Topic-Word-Verteilung, sondern haben das Dokument bereits und suchen die Dokument-Topic-Verteilung. Es wird ein Model gelernt, dass den gegebenen Korpus am wahrscheinlichsten nach folgender Vorgehensweise generiert haben könnte. Dabei sei $Dir(\alpha)$ eine Dirichletverteilung, was einen Parameter $\alpha$ bzw. $\beta$ übergeben bekommt und $Multinom(\theta_d)$ eine Multinomialverteilung mit dem Parameter $\theta$ bzw. $\phi$. Bei der Berechnung ergibt sich der Output durch die dann berechneten Wahrscheinlichkeitsverteilungen $\theta_d$ und $\phi_k$.

\begin{enumerate}
	\item Wähle ein $\theta_d$ als $Dir(\alpha)$
	\item Wähle ein $\phi_k$ als $Dir(\beta)$
	\item Für jedes Wort $w$ and Stelle $i = 1,...,N$ im Dokument $d$: 
	\begin{enumerate}
		\item Wähle ein Thema $z_{d,i}$ als $Multinom(\theta_d)$
		\item Wähle ein Wort $w_{d,i}$ als $Multinom(\phi_z_{d,i})$
	\end{enumerate}
\end{enumerate}

Für das Modell kann die Gesamtwahrscheinlichkeit berechnet werden. Diese ergibt sich aus dem Produkt aus: 
\begin{itemize}
\item der Wahrscheinlichkeit der Dokument-Topic-Verteilung zusammen mit der Dirichlet-Verteilung $\alpha$ für jedes Dokument
\item der Wahrscheinlichkeit der Topic-Word-Verteilung zusammen mit der Dirichlet-Verteilungen $\beta$ für alle Topics
\item der Wahrscheinlichkeit eines Topics zusammen mit der Dokument-Topic-Verteilung für alle Wörter in allen Dokumenten
\item der Wahrscheinlichkeit eines Wortes zusammen mit der Topic-Word-Verteilung für alle Wörter in allen Dokumenten\\

\begin{figure}[h]
\begin{center}
\begin{equation}
P(w,z,\theta, \phi, \alpha, \beta) = \prod_{i=1}^{K} P(\phi_i, \beta) \cdot \prod_{j=1}^{D}P(\theta_i, \alpha) \cdot \prod_{r=1}^{N} P(z_{j,r}\mid\theta_i) \cdot P(w_{j,r} \mid \phi_{j,r}) 
\end{equation}
\end{center}
\caption{Die totale Wahrscheinlichkeit eines LDA Modells $M$}
\label{fig:equ1}
\end{figure}
\\


Die Schwierigkeit des Algorithmus besteht in der Berechnung der $\theta$-Verteilung und entsprechenden Topics der gegebenen Dokumente für latente Variablen. Das heißt bekannt sind das Wort und die Dirichlet-Verteilungen $\alpha$ und $\beta$. Auf Basis dieser gegebenen Informationen wird dann die Wahrscheinlichkeit für deine Topic errechnet, der das Wort angehört. Dazu wird die Dokument-Topic-Verteilung gesucht, die von den Dirichlet-Verteilungen beeinflusst wird. Dies lässt sich durch folgende Wahrscheinlichkeitsverteilung ausdrücken, ist jedoch nur approximierbar und nicht exakt zu bestimmen: \\

\begin{figure}[h]
\begin{center}
\begin{equation}
P(\theta, z \mid w, \alpha, \beta) = \frac{P(\theta, z, w \mid \alpha, \beta)}{P(w \mid \alpha, \beta)}
\end{equation}
\end{center}
\caption{Die bedingte Wahrscheinlichkeit für die Topic und Dokument-Topic-Verteilung gegeben einem Wort und den Dirichlet-Verteilungen $\alpha$ und $\beta$}
\label{fig:equ2}
\end{figure}\\


\subsection{Alpha und Beta}
Die Dirichlet Verteilungen werden durch die beiden Parameter $\alpha$ und $\beta$ bestimmt. Diese formulieren die mathematische Bedeutung der beiden Ziele von LDA, die während der Ausführung versucht werden zu verfolgen:

\begin{enumerate}
	\item Ein Dokument wird so wenigen Themen wie möglich zugewiesen ($\alpha$)
	\item Jedes Thema hat so wenig relevante Wörter wie möglich ($\beta$)
\end{enumerate}

Dabei kann 1. erreicht werden, wenn alle Worte eine Topic wären, was jedoch nicht mit 2. übereinstimmen würde. Für einen erfüllten 2. Satz gibt es nicht die minimale Anzahl an Topics. Die Funktionsweise von verschiedenen $\alpha$-Werten zeigen folgende Abbildungen: 
\\
\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.315]{lda_alpha001.png}
\end{center}
\caption{Je kleiner der $\alpha$-Wert wird, desto eindeutiger dominiert ein Topic das Dokument.} 
\label{fig:img1}
\end{figure}\\
\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.315]{lda_alpha1.png}
\caption{Je größer der $\alpha$-Wert wird, desto gleichmäßiger sind die Topics in dem Dokument beteiligt.}
\label{fig:img2}
\end{center}
\end{figure}\\
In der ersten Abbildung \ref{fig:img1} führt ein kleiner $\alpha = 0.01$ Wert zu einer sehr eindeutigen Dokument-Topic-Verteilung. Das erkennt man daran, dass es nur einen sehr großen blauen Kreis gibt. Jeder Kreis repräsentiert eine Topic und mit der Größe dessen Anteil am Dokument. Befinden sich zwei Kreise zum Teil auf der gleichen Position bedeutet das, dass sich die Wörter in den Topics zu einem gewissen Teil überschneiden. In Abbildung \ref{fig:img1} sind bei den meisten Topics nur die Zahlen und keine Kreise zu erkennen, da die Topics einen sehr kleinen Anteil an dem Dokument haben. Dies ist durch $\alpha$ bedingt. Bei der zweiten Abbildung \ref{fig:img2} hingegen haben wir ein $\alpha = 1$, was eine gleichmäßigere Verteilung zur Folge hat. So sieht man, dass es viele Kreise gibt, die sich zum Teil auch überlappen. Die Größe der Kreise unterscheidet sich nicht so stark voneinander, wie bei Abbildung \ref{fig:img1}, somit sind die Topics bezüglich ihren Anteils an dem Dokument ähnlicher zueinander. Der Trade-off zwischen den beiden Zielen ist der Grund für das Funktionieren von LDA, denn dadurch werden Gruppen mit semantisch eng miteinander verbundenen Wörtern, die oft miteinander vorkommen, gefunden. Nach dem gleichen Prinzip funktioniert die Dirichlet-Verteilung der Topic-Word-Verteilung $\beta$.  

%\begin{center}
%\includegraphics{img1.png}
%\end{center}


\chapter{Konzept} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%KONZEPT

Dieser Abschnitt wird die Vorgehensweise beschreiben, wie unter der Nutzung der Daten der ZVO Topic-Modellierung umgesetzt wird, um Aufschluss über die Qualität der Daten zu erlangen. Dafür werden zuerst die Daten, deren Format und deren Verarbeitung vorgestellt und dann die Herangehensweise geschildert und aufgeschlüsselt.

\section{Daten}
Die ZVO hat 133045 unterschiedlich große Dokumente zur Verfügung gestellt. Die Daten sind in einem Dataframe gespeichert und liegen in folgendem Format vor: 

%\begin{center}
%\includegraphics{daten1}
%\end{center}

\begin{center}
\begin{figure}[h]
\begin{tabular}{ccccccccccc}

\hline
\hline
& filname & subject-message & Abt0 & Abt1 & Abt2 & Abt3 & ... & Abt15 & Abt16 & Abt17\\
\hline
0 & FILE0& content0 &0&0&0&1&...&0&0&0\\
1& FILE1 & content1 &0 &0 &0 &0 &... &1&0&0\\
2& FILE2 & content2 & 1 &0 &0 &0 &...&0 &0&0 \\
3& FILE3 & content3 & 0 & 0  &0 &1 &...&0 &1&0 \\
4& FILE4 & content4 & 0 & 1 &0 &0 &...&0 &0&1 \\
...& ... & ... & ... & ...  &... &... &...&... & ...&... \\
133044& FILE133044 & content133044 & 0 & 0  &1 & 0&...&0 &0&0 \\
\hline
\hline
\end{tabuar}
\caption{Daten TODO...}
\end{figure}
\end{center}

Relevant für die Auswertung sind die subject-message, die die eigentliche Anfrage enthält, und die jeweilige Abteilung, der sie von der ZVO händisch zugeordnet wurde. Die Tabelle verfügt über eine Matrix mit 18 Abteilungen, von denen pro subject-message eine oder mehrere mit einer 1 versehen ist bzw. sind. Dies beschreibt die Abteilung bzw. Abteilungen, der bzw. denen diese Anfrage manuell zugeordnet wurde. Die Daten in subject-message sind bereits bereinigt, also liegen wie in Abbildung \ref{fig:data1} vor:\\

%\begin{center}
%\includegraphics{daten2}
%\end{center}

\begin{figure}[h]
\begin{lstlisting}
OUTPUT: 
'wasser verbraucht amt deutschland ablesung zaehlen strom voll ort luebeck art straße messung verband nummer platz markieren wechsel lieferant stelle verbrauch kunde kunden anrede mann sommer beschwerde schrift allgemein kommunikation datenmanagement fern'
\end{lstlisting}
\caption{So könnte ein Datum der ZVO aussehen} 
\label{fig:data1}
\end{figure}\\

Um die Einträge in eine computer-lesbare Form zu verwandeln, muss ein Dictionary erstellt werden, dass alle Wörter auf eine Anzahl ihrer Vorkommen abbildet. Dafür müssen die Wörter als alleinige Listeneinträge einlesbar sein, wie in Abbildung \ref{fig:data2}: \\

%\begin{center}
%\includegraphics{daten3}
%\end{center}

\begin{figure}[h]
\begin{lstlisting}
OUTPUT split: 
['wasser', 'verbraucht', 'amt', 'deutschland', 'ablesung', 'zaehlen', 'strom', 'voll', 'ort', 'luebeck', 'art', 'straße', 'messung', 'verband', 'nummer', 'platz', 'markieren', 'wechsel', 'lieferant', 'stelle', 'verbrauch', 'kunde', 'kunden', 'anrede', 'mann', 'sommer', 'beschwerde', 'schrift', 'allgemein', 'kommunikation', 'datenmanagement', 'fern']
\end{lstlisting}
\caption{Ein gesplittetes Datum vorbereitet für das Dictionary}
\label{fig:data2}
\end{figure}\\


\subsection{Datenreinigung}
Bevor die Topic-Modellierung auf den Daten durchgeführt werden kann, müssen die Daten einem Prozess unterzogen werden. Dieser beginnt mit der Datenakquise, also der Sammlung bestimmter relevanter Daten. Im Falle der ZVO bedeutet dies, dass es genügend Kundenanfragen gibt, die verarbeitet werden können. Wenn diese Daten bestehen, werden sie auf die relevanten Wörter reduziert, aus denen eine bedeutsame Inferenz von Informationen möglich ist, sodass unter anderem die sogeneannten Stop-Words, also eine Menge von Verbindungswörtern entfernt werden. Es gibt vordefinierte Sammlungen von Stop-Words und es können eigene definiert werden. Das Entfernen von Stop-Words verhindert, dass sich das Topic-Modell an den falschen Wörtern aufhält, die keine aussagekräftige semantische Information in sich tragen. Nachdem die Stop-Words entfernt wurden, ergeben die meisten Sätze keinen inhaltlichen Sinn mehr, wie in Abbildung \ref{fig:data1} zu erkennen. Dies ist jedoch nicht schlimm, da der nächste Schritt der Reinigung den Daten die Reihenfolge entzieht und das Bag of Words Modell erstellt. Dabei werden alle Wörter als eine Menge von unabhängigen Wörtern betrachtet, die sich nur durch ihre Vorkommensanzahl in dem Korpus unterscheiden. Dabei wird die Groß- und Kleinschreibung ebenfalls irrelevant, da es bei einer Menge von Wörtern ohne Reihenfolge auch keine Satzanfänge mehr gibt. Des weiteren besteht die Möglichkeit alle konjugierten bzw. deklinierten Formen eines Wortes zum Infinitiv bzw. Nominativ zusammenzufassen. Dies wird als Lemmatisierung bezeichnet und wurde bei der ZVO nicht angewendet. 

\subsection{Feature Engineering}
Wenn die Daten in der gewünschten Form vorliegen, beginnt der Schritt des Featureengineerings. Für einen Computer sind Wörter nicht so leicht zu verarbeiten, wie Zahlen, weshalb in diesem Schritt eine Quantisierung der Wörter und Überführung dieser in eine zahlenbasierte Form vorgenommen wird. Dabei sind die Features in Form der Spalten des Dataframes bereits passend gegliedert. Für die Darstellung der Dokumente als Menge an Zahlenwerten wird ein Dictionary erstellt, das die Wörter wie in Abbildung \ref{fig:data2} erhält und die Anzahl der Wörter ohne Duplikate in einer Liste, das Dictionary, auflistet. Diese Auflistung der Wörter zusammen mit ihrer Anzahl und laufenden Indexnummer kann als Input für ein LDA Modell verwendet werden. Die Worte werden intern als ID dargestellt, die für einen Menschen nicht zu verstehen ist. Aus diese Grund wird im Algorithmus der id2wordParameter eingeführt, der die Übersetzung zwischen Zahl und Datum darstellt. 

\section{Anwendungsfall ZVO}
Bei der ZVO sollen jährlich händisch aufgenommene Anfragen in Zukunft maschinell klassifiziert werden. Dafür wird im Rahmen des M4KK ein Kundenanfrage-Klassifikator erstellt. Für die korrekte Klassifikation ist die Qualität der vorliegenden Daten von hoher Bedeutung. Die Arbeit verfolgt das Ziel, die Qualität der Daten zu analysieren und semantische Gruppierungen zu analysieren. Dafür wird ein LDA Modell generiert, das als Datengrundlage alle verfügbaren ZVO Daten verwendet. Das Vorgehen ist wie folgt in Abbildung \ref{fig:flowchart} dargestellt:   

% FLOW CHART %%%%%%%
\tikzstyle{rect} = [rectangle, text centered, draw =black]
\tikzstyle{arrow} = [thick,->,>=stealth]

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[node distance = 2cm]

\node (mails) [rect] {Datenerhebung durch Mailanfragen};
\node (zuordnung) [rect, below of = mails] {manuelle Zuordnung Daten zu Abteilungen};
\node (Datenreinigung) [rect, below of = zuordnung] {Datenreinigung};
\node (Feature) [rect, below of = Datenreinigung] {Feature Engineering};
\node (LDA) [rect, below of= Feature] {LDA Modell erstellen};
\node (Schluesselwoerter) [rect, below right of = LDA, node distance = 3cm]{Schlüsselwörter finden};
\node (Topics) [rect, below left of = LDA, node distance = 3cm] {Topics finden};
\node (Matrix) [rect, below of = Schluesselwoerter] {Matrix erstellen};
\node (Ueberschneidungen) [rect, below of = Matrix] {Überschneidungen analysieren};
\node (Topicsana)[rect, below of=Topics]{Topics und Keywords analysieren};

\draw [arrow] (mails) -- (zuordnung);
\draw [arrow] (zuordnung) -- (Datenreinigung);
\draw [arrow] (Datenreinigung) -- (Feature);
\draw [arrow] (Feature) -- (LDA);
\draw [arrow] (LDA) -- (Schluesselwoerter);
\draw [arrow] (LDA) -- (Topics);
\draw [arrow] (Topics) -- (Topicsana);
\draw [arrow] (Schluesselwoerter) -- (Matrix);
\draw [arrow] (Matrix) -- (Ueberschneidungen);

\end{tikzpicture}
\caption{Prozessschritte von der Datenerhebung bis zur Analyse. Die Arbeit verfolgt zwei Ziele: Die Topicanalyse und Zuordnung von Topic und Label.}
\label{fig:flowchart}
\end{center}
\end{figure}

Nach der Erhebung der Daten können die einzelnen Anfragen manuell in die vorgegebenen Abteilungsgruppen eingeteilt werden. Dort wird die Datenreinigung, wie in Abschnitt 3.2 beschrieben vorgenommen. Sind die Daten bereinigt, kann das Feature Engineering beginnen, wonach die Daten für den Computer verständlich formatiert sind. Die Erstellung des LDA Modells wird vorgenommen, sobald der Korpus erstellt ist. Die Modellerstellung ist durch den in Abbildung \ref{fig:code1} gegebenen Pseudocode beschrieben. \\
\\

\begin{figure}[h]
%\begin{algorithm}
\begin{algorithmic}
\STATE data $\leftarrow " "$
\FOR{d in Anzahl Dokumente} $data \leftarrow data + str(d)$\ENDFOR 
\STATE Teile data in einzelne items einer Liste auf
\STATE Erstelle ein Dictionary aus der Liste
\STATE Wandle ID aus Dictionary in Wörter um
\STATE Erstelle den Korpus
\STATE Erstelle das Modell
\STATE Gibt die Topic-Wort-Verteilungen für alle Topics aus
\end{algorithmic}\captionsetup{labelfont={sc,bf}, labelsep=newline}
\caption{Pseudoalgorithmus für die Prozesskette}
%\end{algorithm}
\label{fig:code1}
\end{figure}
\\

\\

Ist das Modell generiert und die Topics auslesbar, können die Daten evaluiert werden. Dazu werden zwei Ansätze verfolgt. Zum Einen werden Topics und zugehörigen Schlüsselwörter untersucht, um u.a. Aufschluss über mögliche Verbesserungspotentiale bei der Datenreinigung oder dem Feature Engineering festzustellen. Als zweites werden die vom Modell erfassten Gruppen betrachtet und den gegebenen manuell klassifizierten Abteilungen zugeordnet. Die folgenden Punkte beschreiben den Prozess im Detail: \\

\begin{enumerate}
\item \textbf{Gruppen und Wörter finden}\\
Ein LDA Modell besteht aus zwei Verteilungen, die die zugrundeliegenden Daten semantisch darstellbar machen: die Dokument-Topic-Verteilung und die Topic-Wort-Verteilung. Als Ausgabe des Modells ist also zu erkennen, welche Topics die Dokumentmenge durchschnittlich hauptsächlich beschreiben und welche Wörter in den Topics jeweils dominant vorkommen. Das Modell kann Topics nicht inhaltlich bennen, sondern nur die Verteilungen darstellen. Somit ist nicht eindeutig, welches Topic welche Abteilung der ZVO darstellt. Dafür betrachten wir die Topic-Wort-Verteilungen und schließen von dieser auf die Qualität der Daten. Ist über die dominanten Wörter in einem Topic zu erkennen, welche Abteilung dieser repräsentiert, scheint das Modell und die Daten gut genug zu sein, um die Daten zu klassifizieren. Sollte die Abteilung nicht an den Wörtern ablesbar sein, sind die Daten nicht optimal für eine Klassifikation geeignet. Dabei ist auch relevant, wie oft die gleichen Wörter in den jeweiligen Topics vorkommen und ob Stop-Words Manipulation eine Option der Qualitätssteigerung darstellt.

\item \textbf{Zuordnung Abteilung zu Topic}\\
Das LDA Modell erstellt aus den Daten 18 Topics. Diese Topics sollten im optimalen Fall sehr ähnlich zu den händisch klassifizierten Abteilungen sein, wenn die vordefinierten Abteilungen eine akkurate Repräsentation der Topics in den Daten darstelle. Ist dies nicht der Fall, kann man auf eine schlechte Klassifikation schließen. Dies kann durch eine schlechte Qualität der Daten als auch durch eine ineffiziente manuelle Einteilung der Topics bedingt sein. Die Fähigkeit, Topics auf Abteilungen zu mappen, gibt Aufschluss über die Qualität der Daten. Für die Zuordnung werden zwei Matrizen verwendet: \lstinline{gruppen_LABEL} und \lstinline{gruppen_LDA}. Die erste sortiert alle Dokumentindizes als Listenelemente in die jeweilige Zeile der Matrix, sodass der Index eines händisch in Abteilung 3 eingeordneten Dokumentes in \lstinline{gruppen_LABEL[4]} zu finden ist. Die Matrix \lstinline{gruppen_LDA} beinhaltet alle Indizes der Dokumente, die vom Modell klassifiziert wurden, in gleicher Struktur. Dafür wird für jedes Dokument, das den Korpus ausmacht, eine dokumentseigene Dokument-Topic-Verteilung errechnet. Das Topic, für das das Dokument am wahrscheinlichsten ist, bestimmt, welcher Teilliste der Dokumentindex angehängt wird. Beide Matrizen verfügen nun über die Indizes der Dokumente in den jeweiligen Topics bzw. Abteilungen und können anhand der einzigartigen Indizes auf Überschneidungen geprüft werden. Die Anzahl der Überschneidungen werden in einer Matrix gespeichert, die jedes Element von \lstinline{gruppen_LDA} auf jedes Element von \lstinline{gruppen_LABEL} abbildet und deren Überschneidung zählt. Eine optimale Zuordnung von Topic auf Abteilung ist möglich, wenn jede Zeile ein Maximum in einer Spalte hat, die nicht auch das Maximum einer anderen Zeile enthält. Ist dies jedoch nicht der Fall, sind die Daten in der aktuellen Form nicht optimal für die Klassifizierung. 
\end{enumerate}


\chapter{Implementierung}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%IMPLEMENTIERUNG
Dieser Abschnitt befasst sich mit der Implementierung des im Abschnitt 3 beschriebenen Konzepts. Dabei wird die Implementierung durch Python Code erklärt und die genaue Vorgehensweise dargestellt. Die Ergebnisse werden gezeigt, jedoch noch nicht analysiert. \\
\section{Topic Modeling Methode}
Zur Untersuchung der Qualität der ZVO-Daten wird in dieser Arbeit die LDA Methode verwendet. Dabei wird nur der Text als bekannt angenommen. Die Topics sind zu Beginn nicht bekannt. Als grundlegendes Topic Modellierungsverfahren findet es Verwendung in einem breiten Anwendungsspektrum. Durch die Bekanntheit von LDA sind bereits viele Pakete und Bibliotheken in Programmierumgebungen vorzufinden und einfach zu implementieren. Seit LDAs Veröffentlichung in 2000 wurde eine umfassende und detailreiche Dokumentation entwickelt, die neben vielen Forenbeiträgen, die Arbeit mit LDA stark erleichtern. Zusätzlich hat LDA in diesem Anwendungsfall den Vorteil, dass es nicht wie zum Beispiel LSA direkt Dokumentähnlichkeiten ausgibt, sondern das Ergebnis in Form einer Matrix dartellt, die Wörter auf Dokumente abbildet. Damit ergeben sich als Werte der Matrix die Topics, denen die Wörter jeweils angehören. Bei LDA Modellen ist die Anzahl der Topics ein individueller Input, durch den sich das Ergebnis schwerwiegend verändern kann. Die Werte in der Matrix können somit von $0$ bis zu der individuellen Anzahl der Topics reichen. Die optimale Anzahl an Topics ist grundsätzlich ein nicht einfaches Problem bei Anwendungen. Im Fall der ZVO werden als Anzahl der Topics 18  gewählt, da dies die Anzahl der bereits erstellten Abteilungen ist. 

\section{Toolauswahl}
Bekannte Frameworks zur Topic-Modellierung implementieren grundsätzlich ähnliche Algorithmen. Betrachtet wurden in dieser Arbeit Mallet, Gensim und Sci-kit Learn.  Wichtige Schritte sind die Vorbereitung der Daten, die Implementierung, die Auswertung und die Visualisierung. Als Bibliothek wird in dieser Arbeit Gensim verwendet, die für die Verarbeitung von unstrukturierten Daten und Anwendung von unüberwachten Algorithmen bekannt ist. Algorithmen, wie \lstinline{word2vec}, \lstinline{LSI} oder \lstinline{LDA} entdecken automatisch Strukturen durch das Prüfen von gemeinsam auftretenden Mustern im Korpus der Trainingsdaten. Viele bekannte Algorithmen sind bereits in diesem Framework implementiert, was die Umsetzung der Topic-Modellierung vereinfacht. Gensim erlangte in der Vergangenheit Bekanntheit durch seine hochoptimierten Implementierungen bekannter Algorithmen und der Schnelligkeit und Verlässlichkeit, mit der diese ausgeführt werden. Außerdem wird Gensim in Python verfasst, was sich sehr gut für Probleme im Bereich des Data Science eignet. 
%\subsection{Gibbs Sampling}
%Gibbs Sampling ist eine Technik, um ein LDA Modell zu trainieren. Für das menschliche Auge kann ein Wort eines Dokuments intuitiv einem Thema zugordnet werden. Für den Computer ist dies schwer, da er die Semantik des Wortes nicht kennt. Mit Gibbs Sampling werden Wörter iterativ Topics zugewiesen basierend auf allen restlichen Wörtern und Dokumenten. Dabei ist zum Einen relevant, was die führende Topic in dem jeweiligen Dokument ist und zum Anderen welcher Topic die gleichen Wörter im gesamten Korpus bereits am häufigsten zugeordnet wurden. Die beiden Beobachtungen werden multipliziert, um die Kombination mit der maximalen Wahrscheinlichkeit zu finden. Um zu verhindern, dass ein Wort einem Topic nicht zugewiesen werden kann, das sich nicht in dem bestimmten Dokument befindet, wird der Anzahl der jeweils vorkommenden Themen im Dokument $\alpha$ und der Anzahl der belegten Themen des gleichen Wörter in anderen Dokumenten $\beta$ addiert.  Wurde ein Wort einem Topic zugewiesen wird das nächste Wort betrachtet. Nun ist zu beachten, dass sich die Verteilungen zur Berechnung jedes mal ändern. Somit haben wir einen iterativen Lernfortschritt bei der Topicverteilung.


%\subsection{Hellinger Distanz}
%In der Wahrscheinlichkeitsrechnung kann die Hellinger Distanz als Maß genutzt werden, um die Ähnlichkeit zweier Verteilungen zu berechnen. Für diese Anwendung bietet sich die Hellinger Distanz an, da sie oft im Kontext von Modellen und Topics verwendet wird. Zusätzlich lässt sie sich gut in Gensim Implementationen einbauen. Neben der Verteilungsvergleich kann auch eine Hellingermatrix erstellt werden, die jedes Topic des einen Modells mit jedem Topic eines zweiten Modells vergleicht. Somit wird ist es möglich die Ähnlichkeit zwischen zwei Modellen festzustellen. Formal errechnet sich die Distanz zwischen zwei diskreten Verteilungen $X$ und $Y$ aus folgender Gleichung: 

%\begin{center}
%\begin{equation}
%	H(X,Y) = \frac{1}{\sqrt{2}} \sqrt{\sum^k_{i=1}(\sqrt{x_i}-\sqrt{y_i})^2}
%\end{equation}
%\end{center}

%n diesem Abschnitt werden verschiedene Möglichkeiten implementiert und analysiert, wie LDA im Sinne der ZVO genutzt werden kann. Dabei ist die Zielfrage, wie am besten für ein unbekanntes Dokument die bestimmte Abteilung gefunden werden kann. Dafür müssen Korpora bestehen, die bereits durch Verteilung definiert sind, sodass die Dokument-Themen Verteilung für das neues Dokument inferiert werden kann. Wenn die Themenverteilung des neuen Dokuments gegeben ist, kann über Vergleiche der Verteilungen mit Dokumenten oder Durschnitten von Korpora eine Abteilung für das Dokument identifiziert werden. 

%\textbf{Ein großer Korpus}\\
%Alle Dokumente ergeben einen Korpus. Dieser gibt eine Dokument-Topic-Verteilung aus, die eine generelle inhaltliche Verteilung der Anfragen darstellt. Neue Dokumente können auf dem Korpus geprüft werden, um eine Dokument-Topic-Verteilung zu inferieren. Diese ergibt eine Abteilung, dem das Dokument zugeordnet werden soll. 

%Ein Gesamtbild der thematischen Aufteilung aller Dokumente kann eine mögliche effektivere Neuverteilung der Kategorien schlussfolgern. Diese Methode stellt die simpleste Form der LDA Anwendung auf die ZVO Daten dar. Dafür kann man zwei verschiedene Ansätze verfolgen: 
%\begin{enumerate}
%	\item Mit 80\% der Daten wird das Modell trainiert. Die letzten 20 \% werden zum Testen verwendet. In diesem Fall wird aus dem unüberwachten LDA-Prinzip ein überwachtes Modell simuliert, da die händisch zugeteilten Abteilungen für die Dokumente bereit bekannt sind. Somit kann man prüfen, wie gut das Modell ist. Dies lässt sich in einer Quote darstellen, die die richtig zugeordneten Dokumente durch die Gesamtanzahl der Testdaten teilt. 
%	\item Mit 80\% der Daten wird das Modell trainiert. In dieser Methode werden jedoch die verwendeten 80\% der Daten verwendet, um zu testen, ob eine Veränderung der Korrektheit zu erkennen ist, wenn die  Trainingsdaten gleichzeitig die Testdaten sind. Dafür wird eine Matrix aufgetellt 
%\end{enumerate}

%\item \textbf{18 Korpora mit Durchschnitt}\\
%Jede Abteilung stellt einen Korpus da, deren Verteilung mit der des neuen Dokuments verglichen wird. In diesem Fall hat jede der 18 Abteilungen eine Dokument-Themen Verteilung, die die Abteilung inhaltlich von den anderen unterscheidet. Dazu wird ein neues Dokument in jedem einzelnen Korpus integriert, um eine Dokument-Themen Verteilung für das neue Dokument auf Basis der korpusabhängigen Wörter-Themen Verteilung zu inferieren. 

%\item \textbf{18 Korpora mit Mehrheitsentscheid}\\
%Jede Abteilung stellt einen Korpus dar, deren Dokumente alle einzeln mit der Verteilung des neuen Dokuments verglichen werden, das dann quantitativ einer Abteilung zugeordnet wird. Die Wörter-Themen Verteilung ist für alle dokumente eines Korpus gleich, während für jedes Dokument eine Dokument-Themen Verteilung errechnet wird. Wenn die Dokument-Themen Verteilung der bestehenden Dokumente und die des neuen Dokuments bereitstehen, können diese auf Ähnlichkeit überprüft werden. Da bereits bekannt ist, welcher Abteilung jedes Dokument angehört, stellen die Top X ähnlichsten Dokumente eine Verteilung der Abteilungen dar, denen das neue Dokument inhaltlich am ähnlichsten ist. 
\end{enumerate}



\section{Umsetzung Konzept}

Wie in Abschnitt 3.2 beschrieben verfolgt die Arbeit zwei Ansätze. Im Folgenden wird die Umsetzung dieser dargelegt: \\
 
\begin{enumerate}
\item
Alle Dokumente werden für die Erstellung eines Korpus genutzt. Der Korpus generiert eine Topic-Verteilung für die Gesamtheit aller Dokumente. Dabei werden zuerst alle Anfragedaten in einen String zusammengefügt, der als Grundlage für das Wörterbuch und den Korpus dient. Um diesen in ein Dictionary, also eine nummerierte Auflistung aller Wörter und dessen Anzahl, zu verwandelt, muss der String in eine Liste mit voneinander getrennten Items gesplittet werden. Hier wird ein Bag of Words Prinzip verfolgt, die Reihenfolge ist somit irrelevant für das Ergebnis des Modells. Aus der Liste wird dann das Dictionary erstellt. Durch den Aufruf des LDA Modells wird aus dem Bag of Words mithilfe des Dictionary eine vorgegebene Anzahl an Themen aus der Wortenge modelliert, basierend auf häufig zusammen auftretenden Wörtern. Dadurch ergibt sich neben einer Verteilung der Topics in einem Modell die Verteilung der Wörter, die ein Topic besonders beeinflussen. Ist das Modell generiert, können die Topics ausgelesen werden. Der Programmcode für die beschreibenen Schritte ist in Abbildung \ref{fig:codeinput} dargestellt.\\
\\

\begin{figure}[H]
\begin{lstlisting}[language=Python, basicstyle=\small]
INPUT:
data = ''

# Zusammenfügen aller Dokumente in einen gorßen String
for x in range(0,106000):
    data += df.at[x,'subject-message']

# Aufteilen des Strings in viele einzelne Wörter
list = data.split()

# Erstellen eines Dictionarys
dictionary = corpora.Dictionary([list])
temp = dictionary[0]
id2word = dictionary.id2token

# Erstellen des Korpus
corpus = [dictionary.doc2bow(text) for text in [list]]

# Erstellen des LDA Modells
lda = LdaModel(corpus, num_topics=18, id2word = id2word)

#Ausgeben aller Topics inklusive Schlüsselwörtern
pprint(lda.print_topics())

\end{lstlisting}
\caption{Der Input zum Aufbau eines LDA Modells}
\label{fig:codeinput}
\end{figure}

Der Input-Code generiert aus allen Daten ein Korpus, der 18 Gruppen in den Daten findet. Jede Topic-Word-Verteilungen hat die Eigenschaft, alle Wörter des Korpus aufzulisten, auch wenn das Topic von gewissen Wörtern keinen Gebrauch macht. Die Topics, die von dem Modell generiert wurden, sind durch die 10 häufigsten Schlüsselwörter und ihren Wahrscheinlichkeiten zum Auftreten definiert. Der Output des in Abbildung \ref{fig:codeinput} definierten Algorithmus ist in Abbildung \ref{fig:codeoutput} gelistet. 

\begin{figure}[H]
\begin{lstlisting}[language=Python]
OUTPUT:
[(0,
  '0.012*"ostholstein" + 0.011*"nachricht" + 0.010*"sierksdorf" + '
  '0.009*"zweckverband" + 0.008*"betreff" + 0.008*"danken" + 0.007*"email" + '
  '0.007*"hra" + 0.007*"datum" + 0.007*"hyperlink"'),
 (1,
  '0.014*"ostholstein" + 0.011*"nachricht" + 0.010*"zweckverband" + '
  '0.009*"sierksdorf" + 0.008*"sitzen" + 0.008*"danken" + 0.007*"wagrienring" '
  '+ 0.007*"lübeck" + 0.007*"betreff" + 0.006*"the"'),
 (2,
  '0.014*"sierksdorf" + 0.013*"zweckverband" + 0.011*"nachricht" + '
  '0.010*"ostholstein" + 0.008*"danken" + 0.008*"betreff" + 0.007*"sitzen" + '
  '0.006*"the" + 0.006*"homepage" + 0.006*"frau"'),
  \end{lstlisting}
  \end{figure}
  \newpage
  \begin{figure}[h]
  \begin{lstlisting}
 (3,
  '0.014*"sierksdorf" + 0.011*"ostholstein" + 0.011*"nachricht" + '
  '0.010*"zweckverband" + 0.010*"betreff" + 0.008*"the" + 0.007*"frau" + '
  '0.006*"danken" + 0.006*"denken" + 0.006*"lübeck"'),
 (4,
  '0.012*"zweckverband" + 0.011*"frau" + 0.011*"sierksdorf" + '
  '0.011*"ostholstein" + 0.009*"nachricht" + 0.008*"betreff" + 0.007*"the" + '
  '0.007*"danken" + 0.006*"lübeck" + 0.006*"öffentlich"'),
 (5,
  '0.015*"zweckverband" + 0.011*"sierksdorf" + 0.011*"ostholstein" + '
  '0.009*"the" + 0.008*"betreff" + 0.007*"lübeck" + 0.007*"danken" + '
  '0.007*"nachricht" + 0.007*"hyperlink" + 0.006*"sitzen"'),
 (6,
  '0.013*"nachricht" + 0.012*"ostholstein" + 0.012*"zweckverband" + '
  '0.010*"betreff" + 0.009*"sierksdorf" + 0.007*"danken" + 0.006*"the" + '
  '0.006*"lübeck" + 0.006*"frau" + 0.005*"hyperlink"'),
 (7,
  '0.012*"ostholstein" + 0.012*"sierksdorf" + 0.011*"zweckverband" + '
  '0.010*"nachricht" + 0.008*"danken" + 0.008*"the" + 0.007*"hra" + '
  '0.007*"email" + 0.007*"betreff" + 0.006*"lübeck"'),
 (8,
  '0.012*"ostholstein" + 0.009*"sierksdorf" + 0.008*"nachricht" + '
  '0.008*"danken" + 0.008*"betreff" + 0.008*"frau" + 0.008*"zweckverband" + '
  '0.008*"hyperlink" + 0.007*"email" + 0.007*"homepage"'),
 (9,
  '0.013*"sierksdorf" + 0.012*"zweckverband" + 0.011*"ostholstein" + '
  '0.009*"nachricht" + 0.009*"danken" + 0.008*"frau" + 0.007*"the" + '
  '0.007*"lübeck" + 0.006*"hyperlink" + 0.006*"sitzen"'),
 (10,
  '0.011*"zweckverband" + 0.011*"nachricht" + 0.010*"sierksdorf" + '
  '0.010*"betreff" + 0.009*"ostholstein" + 0.009*"lübeck" + 0.007*"the" + '
  '0.007*"sitzen" + 0.007*"danken" + 0.007*"hyperlink"'),
 (11,
  '0.013*"sierksdorf" + 0.009*"ostholstein" + 0.009*"zweckverband" + '
  '0.008*"betreff" + 0.008*"nachricht" + 0.007*"danken" + 0.007*"hyperlink" + '
  '0.007*"lübeck" + 0.006*"email" + 0.006*"datum"'),
  \end{lstlisting}
  \end{figure}
  \newpage
  \begin{figure}[h]
  \begin{lstlisting}
 (12,
  '0.013*"ostholstein" + 0.012*"sierksdorf" + 0.009*"nachricht" + '
  '0.009*"zweckverband" + 0.008*"betreff" + 0.008*"danken" + 0.007*"email" + '
  '0.006*"frau" + 0.006*"lübeck" + 0.006*"wagrienring"'),
 (13,
  '0.013*"sierksdorf" + 0.010*"the" + 0.010*"zweckverband" + '
  '0.009*"ostholstein" + 0.009*"nachricht" + 0.008*"danken" + 0.007*"lübeck" + '
  '0.007*"betreff" + 0.007*"frau" + 0.007*"öffentlich"'),
 (14,
  '0.012*"zweckverband" + 0.011*"danken" + 0.011*"sierksdorf" + '
  '0.010*"ostholstein" + 0.010*"betreff" + 0.008*"nachricht" + '
  '0.007*"hyperlink" + 0.007*"gmbh" + 0.006*"lübeck" + 0.006*"kundennummer"'),
 (15,
  '0.014*"ostholstein" + 0.011*"sierksdorf" + 0.008*"zweckverband" + '
  '0.008*"danken" + 0.008*"frau" + 0.007*"betreff" + 0.007*"the" + '
  '0.007*"nachricht" + 0.006*"amtsgericht" + 0.006*"sitzen"'),
 (16,
  '0.014*"zweckverband" + 0.013*"ostholstein" + 0.011*"nachricht" + '
  '0.009*"danken" + 0.008*"sierksdorf" + 0.007*"hyperlink" + 0.007*"lübeck" + '
  '0.006*"sitzen" + 0.006*"the" + 0.006*"betreff"'),
 (17,
  '0.012*"ostholstein" + 0.011*"danken" + 0.011*"sierksdorf" + 0.009*"betreff" '
  '+ 0.009*"zweckverband" + 0.008*"nachricht" + 0.007*"the" + 0.007*"frau" + '
  '0.006*"hyperlink" + 0.006*"sitzen"')]
\end{lstlisting}
\caption{Die Topics und Schlüsselwörter}
\label{fig:codeoutput}
\end{figure}

Das LDA hat erfolgreich auf allen verfügbaren Daten ein Modell gebaut, das die Daten in Topics $[0,17]$ eingeteilt hat. Jedes Topic hat dominante Wörter, die zusammen mit ihren jeweiligen Auftretenswahrscheinlichkeiten aufgeslistet sind. Dabei sind dies nur die Wörter mit der höchsten Vorkommenswahrscheinlichkeit. Jedes Topic wird von allen verfügbaren Wörtern im Dictionary dargestellt, nur zu einem unterschiedlichen Anteil. Die Wahrscheinlichkeiten für das Auftreten von Wörtern liegt im Intervall $[1.5\%,0\%]$. Dabei sind viele Wörter in mehreren Topics als Schlüsselwörter zu finden. \\


\item Die Qualität der Daten und des Klassifikators kann alternativ über die Zuordnung der Topics zu den Abteilungen untersucht werden. Die optimalen Daten werden eine perfekte Zuordnung schaffen, ohne beim Modell eine Verwirrung festzustellen. Dafür wird in dieser Arbeit untersucht, wie gut sich die Topics auf die Abteilungen zuordnen lassen. Der wohl naivste Ansatz ist, Topic $x$ auf Abteilung $x$ für alle $x \in [0,17]$ abzubilden. Dafür ist die Anzahl der Dokumente pro Topic bzw. Abteilung relevant. Dabei wird nur die Anzahl der zugeordneten Dokumente überprüft, indem für jedes auf Basis des Korpus eine Dokument-Topic-Verteilung generiert wird. Bei dieser wird vom Algorithmus die Topic, für die die höchste Wahrscheinlichkeit errechnet wurde, für das Dokument ausgewählt. Um welche Dokumente es sich jedoch bei dem Vergleich jeweils tatsächlich handelt ist zu diesem Punkt noch irrelevant. Überschneidungen werden noch nicht betrachtet. In Abbildung \ref{fig:count1} ist die Anzahl der Dokumente in den Topics dargestellt:

\begin{figure}[H]
\begin{center}
\begin{tabular}{cc}
\hline 
\hline
Topic&Counts LDA\\
\hline
T0&8605\\
T1&3959\\
T2&5974\\
T3&1714\\
T4&5823\\
T5&2291\\
T6&23558\\
T7&12721\\
T8&4207\\
T9&1100\\
T10&10021\\
T11&3243\\
T12&14556\\
T13&2697\\
T14&29456\\
T15&1162\\
T16&575\\
T17&1400\\
\hline
\hline
\end{tabular}
\caption{Anzahl Dokumente in den Topics}
\label{fig:count1}
\end{center}
\end{figure}
\\
Dasselbe ergibt sich für die Abteilungen und die Anzahl derer Dokumente. Dafür wird in dem von der ZVO bereitgestellten Datensatz die Zuordnung für jedes Dokument abgefragt. Dadurch kann das Dokument akkurat der Topic zugewiesen werden, wie es händisch getan wurde. Dies ist in Abbildung \ref{fig:count2} zu erkennen. \\ 

\begin{figure}[H]
\begin{center}
\begin{tabular}{cc}
\hline
\hline
Abteilung & Counts Label\\
\hline
L0&9770\\
L1&21061\\
L2&14245\\
L3&235\\
L4&1307\\
L5&251\\
L6&4009\\
L7&3610\\
L8&9410\\
L9&23533\\
L10&20676\\
L11&43\\
L12&2866\\
L13&9616\\
L14&8144\\
L15&12126\\
L16&4748\\
L17&2700\\
\hline
\hline
\end{tabular}
\caption{Anzahl Dokumente in den Abteilungen}
\label{fig:count2}
\end{center}
\end{figure}

Bei diesem Ansatz muss bedacht werden, dass ein Dokument von der ZVO in mehrere Abteilungen eingeteilt werden kann, während der Algorithmus in diesem Fall das Dokument immer nur einem Topic zuordnen. Da die Mehrheit der Dokument jedoch nur einer Abteilung zugewiesen ist, entsteht hier nur eine kleine Tendenz, die die Daten der ZVO in Abbildung \ref{fig:count2} etwas mehr Dokumente zuordnet, als in Abbildung \ref{fig:count1}.\\

In Abbildung \ref{fig:count2} sind die Abteilungen über Labels von $0$ bis $17$ aufgelistet. Dabei dient der Begriff Label der Adressierung einer Abteilung. So stellt Label 0 die Abteilung 0 der ZVO dar. Im Anwendungsfall steht jedes Label damit für eine reale Abteilung. Die Zuordnung der Labels zu Abteilungen ist in Tabelle \ref{fig:labels} dargestellt.


\begin{figure}[h]
\begin{center}
\begin{tabular}{ll}
\hline
\hline
Label&Abteilung\\
\hline
L0&Ablesung\\
L1&Allgemeiner Schriftverkehr\\
L2&BM-Beschwerden\\
L3&KA-Kommunaler Ausfall\\
L4&MaKo-Klärfälle\\
L5&ZSB-Aktionen\\
L6&ZSB-BAV\\
L7&ZSB-Zählerdatenmanagement\\
L8&ZSC-Forderungsmanagement\\
L9&ZSK-An/Abmeldungen\\
L10&ZSK-Bank\\
L11&ZSK-Interessenten\\
L12&ZSK-Korrekturrechnungen\\
L13&ZSK-Schwierige Fälle\\
L14&ZSK-Stammdaten/Postrückläufer
L15&ZSK-Standard Abfall\
L16&ZSK-Standard Verbrauch\\
L17&ZSK-Widersprüche Abfall\\
\hline
\hline
\end{tabular}
\caption{Die Zuordnung von Label zu Abteilung}
\label{fig:labels}
\end{center}
\end{figure}\\


Die Anzahl der Dokumente eines Topics kann nun mit der einer Abteilung verglichen werden, um die grundsätzliche Kompatibilität zwischen Topics und Abteilungen prüfen zu können. Dies gibt jedoch nur eine grobe Richtung an, da die Dokument selbst nicht geprüft werden. Das bedeutet, dass die verglichenen Anzahlen nicht prüfen, ob es sich sowohl in der Topic, als auch in der Abteilung auch wirklich um das gleiche Dokument handelt. Um die Dokumente mit in den Vergleich einzubeziehen und damit die Zuordnung von Topic auf Abteilung zu verbessern, wird eine weitere Methode betrachtet. Die Zuordnung der Topics zu Abteilungen kann approximiert werden, indem gezählt wird, in wie vielen Dokumenten sich die Topics und Abteilungen jeweils überschneiden. Genau wie bei der Gruppeneinteilung wird für diese Überschneidung jedes Dokument erneut auf dem Korpus geprüft, um dessen Topic-Wort-Verteilung zu erstellen. Die Dokumente werden anhand ihrer einzigartigen Zeilen-ID adressiert. Die IDs werden der Topic zugeordnet, die die höchste Wahrscheinlichkeit in der Verteilung des Dokuments erreicht. Die Zuordnung findet in einer großen Liste statt, die über 18 Unterlisten verfügt, in die die IDs jeweils hinzugefügt werden. Die Überschneidungen werden in einer Matrix dargestellt, die für jedes Topic des Modells die Überschneidungen zu jeder Abteilung auflistet. Somit kann für jede Zeile die passende Zuordnung gefunden werden. Dabei stellt eine Zeile jeweils die Dokumente dar, die aufsummiert die Zahl einer Zeile in Abbildung \ref{fig:count1} ergeben. 

%comp bei 10000: [[536, 91, 67, 1, 12, 1, 12, 6, 23, 145, 69, 0, 22, 12, 38, 87, 24, 6], [15, 22, 11, 0, 2, 0, 0, 1, 7, 32, 77, 0, 6, 3, 5, 16, 3, 1], [6, 50, 81, 1, 3, 1, 31, 6, 28, 61, 30, 0, 18, 15, 22, 33, 4, 16], [14, 69, 58, 3, 3, 0, 19, 11, 37, 104, 63, 0, 18, 20, 21, 51, 33, 5], [7, 81, 104, 1, 3, 0, 7, 9, 56, 112, 97, 1, 11, 28, 36, 42, 15, 12], [0, 4, 3, 1, 0, 0, 1, 0, 2, 5, 3, 0, 1, 1, 0, 1, 0, 0], [2, 7, 24, 0, 0, 0, 2, 0, 7, 25, 23, 0, 2, 5, 2, 13, 8, 1], [19, 331, 90, 3, 16, 3, 49, 22, 111, 206, 200, 2, 29, 49, 61, 127, 36, 44], [3, 20, 33, 0, 1, 0, 2, 1, 11, 41, 78, 0, 3, 5, 7, 15, 8, 1], [4, 21, 17, 0, 2, 0, 1, 2, 26, 18, 33, 0, 3, 9, 5, 4, 2, 0], [13, 137, 115, 0, 6, 0, 25, 16, 53, 183, 136, 3, 6, 34, 75, 57, 39, 9], [27, 429, 221, 8, 34, 4, 129, 34, 291, 312, 271, 2, 48, 137, 75, 149, 50, 106], [0, 12, 8, 0, 1, 0, 3, 2, 5, 24, 13, 0, 2, 1, 3, 10, 5, 1], [12, 89, 73, 0, 9, 1, 9, 18, 25, 106, 124, 0, 4, 14, 29, 69, 52, 5], [10, 174, 110, 4, 11, 1, 15, 5, 42, 226, 137, 1, 19, 38, 39, 181, 21, 27], [8, 67, 70, 1, 4, 0, 11, 4, 21, 85, 115, 0, 8, 16, 20, 47, 21, 14], [19, 33, 44, 0, 5, 0, 4, 4, 18, 56, 63, 0, 6, 9, 16, 18, 7, 3], [39, 47, 52, 0, 5, 1, 15, 10, 203, 74, 71, 1, 12, 13, 16, 33, 17, 6]]

\begin{figure}[H]
\begin{center}
\tiny
\renewcommand{\arraystretch}{2}
\begin{tabular}{ccccccccccccccccccc}
\hline
\hline 
&L0&L1&L2&L3&L4&L5&L6&L7&L8&L9&L10&L11&L12&L13&L14&L15&L16&L17\\
\hline
\hline
T0&555& 977& \textbf{2150}& 21& 66& 19& 211& 251& 910& 960& 902& 2& 210& 596& 302& 758& 318& 250\\
\hline 
T1&512& 580& \textbf{650}& 3& 36& 7& 139& 101& 370& 453& 570& 0& 66& 378& 91& 196& 123& 53\\
\hline 
T2&508& 755& 263& 9& 67& 21& 254& 145& 282& \textbf{1498}& 1207& 1& 119& 300& 479& 527& 177& 130\\
\hline 
T3&108& 179& 98& 2& 15& 3& 33& 57& 236& \textbf{432}& 218& 0& 34& 55& 153& 198& 74& 27\\
\hline 
T4&493& 867& 442& 5& 63& 8& 134& 180& 451& \textbf{1111}& 1033& 8& 73& 348& 446& 489& 191& 89\\
\hline 
T5&91& 467& 98& 5& 25& 2& 60& 75& 162& \textbf{480}& 309& 1& 55& 127& 169& 351& 113& 47\\
\hline 
T6&1263& 3993& 1738& 28& 127& 48& 811& 587& 1384& 4139& \textbf{4730}& 6& 573& 2096& 1620& 2080& 680& 495\\
\hline 
T7&\textbf{3376}& 1298& 870& 17& 524& 16& 221& 338& 662& 1904& 1740& 9& 185& 381& 664& 891& 537& 133\\
\hline 
T8&310& 585& 216& 10& 27& 5& 67& 114& 203& 771 & \textbf{1013}& 0& 81& 323& 264& 440& 205& 73\\
\hline 
T9&58& 126& 47& 0& 3& 1& 17& 16& 131& 174& \textbf{400}& 0& 15& 55& 63& 69& 59& 17\\
\hline 
T10&286& \textbf{1610}& 1733& 12& 51& 27& 284& 299& 858& 1567& 1331& 4& 239& 893& 674& 673& 304& 248\\
\hline 
T11&194& 409& 377& 8& 27& 3& 88& 122& 180& \textbf{678}& 470& 0& 80& 173& 167& 395& 142& 118\\
\hline 
T12&486& 2355& 2495& 16& 80& 24& 564& 327& 1005& \textbf{2672}& 1513& 3& 311& 1254& 886& 1377& 579& 327\\
\hline 
T13&151& 393& 198& 5& 23& 4& 64& 83& 160& 414& \textbf{639}& 1& 68& 192& 123& 275& 151& 68\\
\hline 
T14&1008& 6084& 2681& 86& 153& 61& 990& 820& 2223& \textbf{5730}& 3832& 8& 701& 2327& 1860& 3093& 993& 552\\
\hline 
T15&270& 95& 36& 3& 10& 1& 29& 32& 46& 228& \textbf{284}& 0& 19& 31& 61& 81& 23& 21\\
\hline 
T16&81& 82& 25& 1& 3& 0& 18& 38& 27& \textbf{105}& 90& 0& 14& 29& 37& 76& 24& 15\\
\hline
T17&20& 206& 128& 4& 7& 1& 25& 25& 120& \textbf{217}& 395& 0& 23& 58& 85& 157& 55& 37\\
\hline
\hline
\end{tabular}
\caption{Anzahl der Überschneidungen zwischen LDA Topics (vert.) und ZVO Labels (horiz.) mit den maximalen Werten pro Zeile in \textbf{fett} markiert.}
\label{fig:matrixnormal}
\end{center}
\end{figure}\\


%[[555, 977, 2150, 21, 66, 19, 211, 251, 910, 960, 902, 2, 210, 596, 302, 758, 318, 250], [512, 580, 650, 3, 36, 7, 139, 101, 370, 453, 570, 0, 66, 378, 91, 196, 123, 53], [508, 755, 263, 9, 67, 21, 254, 145, 282, 1498, 1207, 1, 119, 300, 479, 527, 177, 130], [108, 179, 98, 2, 15, 3, 33, 57, 236, 432, 218, 0, 34, 55, 153, 198, 74, 27], [493, 867, 442, 5, 63, 8, 134, 180, 451, 1111, 1033, 8, 73, 348, 446, 489, 191, 89], [91, 467, 98, 5, 25, 2, 60, 75, 162, 480, 309, 1, 55, 127, 169, 351, 113, 47], [1263, 3993, 1738, 28, 127, 48, 811, 587, 1384, 4139, 4730, 6, 573, 2096, 1620, 2080, 680, 495], [3376, 1298, 870, 17, 524, 16, 221, 338, 662, 1904, 1740, 9, 185, 381, 664, 891, 537, 133], [310, 585, 216, 10, 27, 5, 67, 114, 203, 771, 1013, 0, 81, 323, 264, 440, 205, 73], [58, 126, 47, 0, 3, 1, 17, 16, 131, 174, 400, 0, 15, 55, 63, 69, 59, 17], [286, 1610, 1733, 12, 51, 27, 284, 299, 858, 1567, 1331, 4, 239, 893, 674, 673, 304, 248], [194, 409, 377, 8, 27, 3, 88, 122, 180, 678, 470, 0, 80, 173, 167, 395, 142, 118], [486, 2355, 2495, 16, 80, 24, 564, 327, 1005, 2672, 1513, 3, 311, 1254, 886, 1377, 579, 327], [151, 393, 198, 5, 23, 4, 64, 83, 160, 414, 639, 1, 68, 192, 123, 275, 151, 68], [1008, 6084, 2681, 86, 153, 61, 990, 820, 2223, 5730, 3832, 8, 701, 2327, 1860, 3093, 993, 552], [270, 95, 36, 3, 10, 1, 29, 32, 46, 228, 284, 0, 19, 31, 61, 81, 23, 21], [81, 82, 25, 1, 3, 0, 18, 38, 27, 105, 90, 0, 14, 29, 37, 76, 24, 15], [20, 206, 128, 4, 7, 1, 25, 25, 120, 217, 395, 0, 23, 58, 85, 157, 55, 37]] MIT 133044 Dokumenten!!!!

Diese Matrix bildet die Überschneidungen aller 133044 Dokumenten ab. Dabei sind die 18 LDA Topics auf der vertikalen Achse und die 18 LABEL Abteilungen auf der horizontalen Achse aufgetragen. Bezüglich der Zuordnung ist der naivste Ansatz, jeder Zeile (also LDA Topic) die Spalte (also Label Abteilung) mit der maximalen Überschneidung zuzuordnen. Dabei ist das Ziel die Anzahl der Gesamtüberschneidungen zu maximieren. \\


\end{enumerate}

	
\chapter{Analyse}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%ANALYSE
Die Implementierung des LDA Modells wurde im vorausgegangenen Teil vorgenommen. In diesem Abschnitt werden die Ausgaben analysiert und aus verschiedenen Perspektiven auf Erkenntnisse geprüft. Wie in Abschnitt 3 und 4 wird auch dieser Abschnitt in die zwei Teilbereiche dieser Arbeit unterteilt.

\begin{enumerate}
\item \textbf{Gruppen und Wörter finden}\\
Das LDA Modell hat 18 Gruppen aus den Daten generiert und diese mit einer Verteilung von Schlüsselwörtern beschrieben. Als Anwendungsziel in Bezug auf die ZVO sollen die Topics des Modells in Zukunft die Abteilungen darstellen. Betrachtet man dafür die Wörter der Topics, um die Topics semantisch den Abteilungen zuzuordnen, wird dies nicht gelingen. Das liegt daran, dass sich die dominaten Wörter in den Topics zu stark überschneiden. Zum Beispiel gehört ostholstein in allen Topics zu den häufigsten 5 Wörtern. In der folgenden Darstellung sind die häufigsten 5 Wörter aufgelistet und mit den 18 Topics auf Vorkommen geprüft: 

\begin{figure}[H]
\begin{center}
%\tiny
%\renewcommand{\arraystretch}{2}
%\begin{tabular}{ccccccccccccccccccc}
%Wort&L0&L1&L2&L3&L4&L5&L6&L7&L8&L9&L10&L11&L12&L13&L14&L15&L16&L17&L18\\
\begin{tikzpicture}
  \begin{axis}[title  = Topicabdeckung der häufigsten 5 Schlüsselwörter,
    xbar,
    y axis line style = { opacity = 0 },
    axis x line       = none,
    tickwidth         = 0pt,
    ytick             = data,
    enlarge y limits  = 0.05,
    enlarge x limits  = 0.02,
    width=0.7\textwidth,,
    bar width=5.5mm,    symbolic y coords = {oosthosltein,sierksdorf,zweckverband,nachricht,betreff,danken,frau,the,sitzen}, nodes near coords
  ]
  \addplot coordinates { (18,ostholstein)(17,zweckverband)(18,sierksdorf)(14,nachricht)(10,betreff)(8,danken)(2,frau)(2,the)(1,sitzen)};
  \end{axis}
\end{tikzpicture}
\caption{Topicabdeckung der häufigsten 5 Wörter aller Topics}
\label{fig:top5}
\end{center}
\end{figure}

In Abbildung \ref{fig:top5} sind die häufigsten 5 Wörter aller Topics auf der Y-Achse aufgelistet. Die X-Achse beschreibt die Anzahl der Topics, die das bestimmte Wort als eines der häufigsten 5 besitzen. Hätten alle 18 Topics Schlüsselwörter, die das Topic eindeutig beschreiben, würden sich 90 verschiedene Wörter auf der Y-Achse ergeben. Hier sind jedoch nur 9 Wörter aufgezählt, deren Anzahl sich auf 90 addiert. Die Topics teilen sich also auf 90 möglichen Schlüsselwörtern nur 9 Stück. Das bedeutet, dass sich die Topics semantisch sehr schwach bis garnicht von einander unterscheiden lassen. Dies ist auf die Qualität der Daten und dessen Wörter zurückzuführen. Die Topics lassen sich nicht stark genug von einander abgrenzen, wenn in jeden Topic die selben Wörter zu einem hohen Anteil vorkommen. Somit kann die Qualität der Daten und damit die des Klassifikators erhöht werden, indem die Stoppwörter erweitert werden. Wenn man beispielsweise das Wort ostholstein betrachtet, ist dies nicht aussagekräftig für die Abteilung, da sich alle Abteilungen in Ostholstein befinden. Die Wahl der zu ergänzenden Stoppwörter hängt also mit von der semantischen Bedeutung des Wortes ab. Dazu muss auf die Anzahl der Wörter insgesamt geachtete werden und die stark vertretenen Worte dieser Art entfernt werden. Zuletzt ist bedeutend, wie stark die semantische Bedeutung des jeweiligen Wortes für die bestimmte Abteilung ist. Ein Wort, das eine ganz bestimmt Abteilung beschreibt, wie ablesen die Abteilung Ablesung, darf dies natürlich nicht entfernt werden. 
\\

%\begin{figure}[H]
\begin{center}
\begin{tikzpicture} 
\begin{axis} [tick label style={/pgf/number format/fixed}, scaled ticks=false,ybar,xmin=-1,xmax=18,ymin=0,ymax=30000, bar width=8pt, height=8cm,width=15cm, ylabel={Anzahl Elemente}, xlabel={Topics bzw. Abteilungen}]
\addplot coordinates {
    (0,8605)
    (1,3959)
    (2,5974)
    (3,1714)
    (4,5823)
    (5,2291)
    (6,3558)
    (7,12721)
    (8,4207)
    (9,1100)
    (10,10021)
    (11,3243)
    (12,14556)
    (13,2697)
    (14,29456)
    (15,1162)
    (16,575)
    (17,1400)
};
\addplot coordinates {
(0,9770)
(1,21061)
(2,14245)
(3,235)
(4,1307)
(5,251)
(6,4009)
(7,3610)
(8,9410)
(9,23533)
(10,20676)
(11,43)
(12,2866)
(13,9616)
(14,8144)
(15,12126)
(16,4748)
(17,2700)
};
\legend{LDA, Label};
\end{axis}
\caption{Vergleich der Anzahl an Dokumenten pro Topic bzw. Abteilung}
\label{fig:countschart}
\end{tikzpicture}
\end{center}
%\end{figure}

Das Säulendiagramm in Abbildung \ref{fig:countschart} bildet jeweils für alle $x in [0,17]$ Abteilung x auf Topic x ab und stellt die Anzahl in diesen Topics bzw. Abteilungen gegenüber. Dies zeigt, dass die Topics nicht optimal auf die Abteilungen abgebildet sind. Die Anzahl der enthaltenen Dokumente sollte ähnlicher sein. Betrachtet man zum Beispiel Topic 14, kann diese nicht Abteilung 14 darstellen, da es sich um einen Unterschied von 21312 Dokumenten, also 72$\%$, handelt.\\

In den Abbildungen \ref{fig:count1} und \ref{fig:count2} ist die Anzahl der Dokumente in den jeweiligen Topics und Abteilungen aufgelistet. Um eine bessere Übersicht über die Daten zu bekommen, sind die Listen in den folgenden Abbildungen nach der Größe sortiert und mit einem weiteren Parameter dargestellt, der als Vergleichswert dienen soll. Dieser Parameter stellt den prozentualen Anteil der Topics im Korpus dar: 

\begin{figure}[H]
\begin{center}
\begin{tabular}{ccc}
\hline 
\hline
Topics & Counts LDA & Anteil$[\%]$\\
\hline
14&29456&22.13\\
6&23558&17.7\\
12&14556&10.94\\
7&12721&9.56\\
10&10021&7.53\\
0&8605&6.47\\
2&5974&4.49\\
4&5823&4,38\\
8&4207&3.16\\
1&3959&2.98\\
11&3243&2.44\\
13&2697&2.03\\
5&2291&1.72\\
3&1714&1.29\\
17&1400&1.05\\
15&1162&0.87\\
9&1100&0.83\\
16&575&0.43\\
\hline
\hline
\end{tabular}
\caption{Anzahl Dokumente in Topics mit relativem Anteil}
\end{center}
\end{figure}

%\quad

\begin{figure}[H]
\begin{center}
\begin{tabular}{ccc}
\hline
\hline
Abteilung & Counts Label & Anteil$[\%]$\\
\hline
9&23533&15.86\\
1&21061&14.20\\
10&20676&13.94\\
2&14245&9.6\\
15&12126&8.17\\
0&9770&6.59\\
13&9616&6.48\\
8&9410&6.34\\
14&8144&5.49\\
16&4748&3.20\\
6&4009&2.7\\
7&3610&2.43\\
12&2866&1.93\\
17&2700&1.82\\
4&1307&0.88\\
5&251& 0.17\\
3&235&0.16\\
11&43&0.03\\
\hline
\hline
\end{tabular}
\caption{Anzahl Dokumente in Abteilungen mit relativem Anteil}
\end{center}
\end{figure}


Nun wird durch die quantitativere Darstellung deutlich, dass sich nicht alle Topics eindeutig einer Abteilung zuordnen lassen. Das Ergebnis zeigt, dass die Topic-Verteilung bei LDA das 14. Thema stärker erkennt, als die ZVO die Abteilung mit den meisten zugeordneten Dokumenten. Diese unterschieden sich durch $5923$ Dokumente. Die Betrachtung der unterschiedlichen Anteile dient somit nicht gut als Zuordnungsmethode, kann jedoch Aufschluss darüber geben, dass die Daten von der ZVO gleichmäßiger zugeteilt wurden, als sie von dem Topic-Modelling erkannt werden. Dies bedeutet, dass die Wörter in den Dokumenten in ihrer Bedeutung und syntaktischen Umgebung zu ähnlich sind, als dass sie sich optimal in die von der ZVO vorgegebenen Abteilungen einteilen lassen. Dies könnte auch an bestimmten Wörtern liegen, die in vielen Abteilungen vorkommen, wie zu, Beispiel Ostholstein oder Zweckverband. Die Wörter können dafür sorgen, dass Dokumente in die gleiche Topic sortiert werden, die sich eigentlich durch andere Wörter semantisch stärker voneinander unterscheiden und manuell anderen Abteilungen untergeordnet wurden.\\ 

\item \textbf{Zuordnung Abteilung zu Topic}\\
%Die durchschnittliche Überschneidung zwischen Abteilung und Topic spiegelt wieder, wie stark die Abteilung mit einem oder mehreren Topics übereinstimmt. Ein hoher Durschnitt kann entweder eine sehr hohe Überschneidung der Abteilung mit einer Topic oder relativ hohe Überschneidungen mit mehreren Topics bedeuten. Für die Umsetzung betrachten wir die Abbildung \ref{fig:matrixnormal} aus Abschnitt 4, die eine Matrix mit allen Überschneidungen zwischen Topics und Abteilungen mit absoluten Werten aufzeichnet. Hierbei beinhalten die Werte nur Dokumente, die sowohl in der Topic, als auch in der Abteilung enthalten sind. Um eine möglichst effektive Zuteilung zu erlangen, wählen wir zuerst den naivsten Ansatz, aus jeder Zeile den maximalen Wert zu nehmen. Als Graphik ist das wie folgt darzustellen: 

Für die Zuordnung der Topics auf Abteilungen wurde in Abschnitt 4 in Abbildung \ref{fig:matrixnormal} eine Matrix erstellt, die alle Überschneidungen gezählt hat. Diese wurden dann für alle Topics und Abteilungen in der Matrix als absolute Werte aufgetragen. Abbildung \ref{fig:maxgraph} stellt die Ergebnisse der Matrix in einem Graphen dar, auf dem die X-Achse die Labels, also Abteilungen, aufträgt und die Y-Achse für die Anzahl der überschneidenden Dokumente steht. Wie viele Dokumente sich in jedem Topic und Abteilung überschneiden ist durch die 18 Graphen dargestellt, die jeweils den Wert in einer Funktion aufgetragen haben. Aus Abbildung \ref{fig:matrixgraph} ist ein globaler Trend zu erkennen, der bei den Abteilungen $1,2$ und $9,10$ sehr viele Überschneidungen aufweist. 

%\begin{figure}[h]
\begin{center}
\begin{tikzpicture}
\begin{axis}[
width=15cm,height=15cm,
    title={Überschneidungen aller Topics mit allen Labels},
    xlabel={Labels},
    ylabel={Anzahl Überschneidugen},
    xmin=-1, xmax=18,
    ymin=0, ymax=5000,
    xtick={0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18},
    ytick={0,500,1000,1500,2000,2500,3000,3500,4000,4500,5000},
    %legend pos=north east,
    legend style ={at={ (0.23,0.315) }, anchor=south west},
    ymajorgrids=true,
    grid style=dashed,
]

\addplot[
    color=blue,
    mark=square,
    ]
    coordinates {
    (0,555)(1,977)(2,2150)(3,21)(4,66)(5,19)(6,211)(7,251)(8,910)(9,960)(10,902)(11,2)(12,210)(13, 596)(14,302)(15,758)(16,318)(17,250)
    };
\addplot[
    color=blue!95,
    mark=square,
    ]
    coordinates {
    (0,512)(1, 580)(2, 650)(3, 3)(4, 36)(5, 7)(6, 139)(7, 101)(8, 370)(9, 453)(10, 570)(11, 0)(12, 66)(13, 378)(14, 91)(15, 196)(16, 123)(17, 53)};

\addplot[
    color=blue!90,
    mark=square,
    ]
    coordinates {(0,508)(1, 755)(2, 263)(3, 9)(4, 67)(5, 21)(6, 254)(7, 145)(8, 282)(9, 1498)(10, 1207)(11, 1)(12, 119)(13, 300)(14, 479)(15, 527)(16, 177)(17, 130)};
 
 \addplot[
    color=blue!85,
    mark=square,
    ]
    coordinates {(0,108)(1, 179)(2, 98)(3, 2)(4, 15)(5, 3)(6, 33)(7, 57)(8, 236)(9, 432)(10, 218)(11, 0)(12, 34)(13, 55)(14, 153)(15, 198)(16, 74)(17, 27)};
 
 \addplot[
    color=blue!80,
    mark=square,
    ]
    coordinates {(0,493)(1, 867)(2, 442)(3, 5)(4, 63)(5, 8)(6, 134)(7, 180)(8, 451)(9, 1111)(10, 1033)(11, 8)(12, 73)(13, 348)(14, 446)(15, 489)(16, 191)(17, 89)};
    
\addplot[
    color=blue!75,
    mark=square,
    ]
    coordinates {(0,91)(1, 467)(2, 98)(3, 5)(4, 25)(5, 2)(6, 60)(7, 75)(8, 162)(9, 480)(10, 309)(11, 1)(12, 55)(13, 127)(14, 169)(15, 351)(16, 113)(17, 47)};

\addplot[
    color=blue!70,
    mark=square,
    ]
    coordinates {(0,1263)(1, 3993)(2, 1738)(3, 28)(4, 127)(5, 48)(6, 811)(7, 587)(8, 1384)(9, 4139)(10, 4730)(11, 6)(12, 573)(13, 2096)(14, 1620)(15, 2080)(16, 680)(17, 495)};
 
\addplot[
    color=blue!65,
    mark=square,
    ]
    coordinates {(0,3376)(1, 1298)(2, 870)(3, 17)(4, 524)(5, 16)(6, 221)(7, 338)(8, 662)(9, 1904)(11, 1740)(12, 9)(13, 185)(14, 381)(15, 664)(16, 891)(17, 537)(18, 133)};
    
\addplot[
    color=blue!60,
    mark=square,
    ]
    coordinates {(0,310)(1, 585)(2, 216)(3, 10)(4, 27)(5, 5)(6, 67)(7, 114)(8, 203)(9, 771 )(10, 1013)(11, 0)(12, 81)(13, 323)(14, 264)(15, 440)(16, 205)(17, 73)};
    
\addplot[
    color=blue!55,
    mark=square,
    ]
    coordinates {(0,58)(1, 126)(2, 47)(3, 0)(4, 3)(5, 1)(6, 17)(7, 16)(8, 131)(9, 174)(10, 400)(11, 0)(12, 15)(13, 55)(14, 63)(15, 69)(16, 59)(17, 17)};
    
\addplot[
    color=blue!50,
    mark=square,
    ]
    coordinates {(0,286)(1, 1610)(2, 1733)(3, 12)(4,51)(5, 27)(6, 284)(7, 299)(8, 858)(9, 1567)(10, 1331)(11, 4)(12, 239)(13, 893)(14, 674)(15, 673)(16, 304)(17, 248)};
    
\addplot[
    color=blue!45,
    mark=square,
    ]
    coordinates {(0,194)(1, 409)(2, 377)(3, 8)(4, 27)(5, 3)(6, 88)(7, 122)(8, 180)(9, 678)(10, 470)(11, 0)(12, 80)(13, 173)(14, 167)(15, 395)(16, 142)(17, 118)};
    
\addplot[
    color=blue!40,
    mark=square,
    ]
    coordinates {(0,486)(1, 2355)(2, 2495)(3, 16)(4, 80)(5, 24)(6, 564)(7, 327)(8, 1005)(9, 2672)(10, 1513)(11, 3)(12, 311)(13, 1254)(14, 886)(15, 1377)(16, 579)(17, 327)};
    
\addplot[
    color=blue!35,
    mark=square,
    ]
    coordinates {(0,151)(1,393)(2, 198)(3, 5)(4, 23)(5, 4)(6, 64)(7, 83)(8, 160)(9, 414)(10, 639)(11, 1)(12, 68)(13, 192)(14, 123)(15, 275)(16, 151)(17, 68)};
    
\addplot[
    color=blue!30,
    mark=square,
    ]
    coordinates {(0,1008)(1, 6084)(2, 2681)(3, 86)(4, 153)(5, 61)(6, 990)(7, 820)(8, 2223)(9, 5730)(10, 3832)(11, 8)(12, 701)(13, 2327)(14, 1860)(15, 3093)(16, 993)(17, 552)};
 
\addplot[
    color=blue!25,
    mark=square,
    ]
    coordinates {(0,270)(1, 95)(2, 36)(3, 3)(4, 10)(5, 1)(6, 29)(7, 32)(8, 46)(9, 228)(10, 284)(11, 0)(12, 19)(13, 31)(14, 61)(15, 81)(16, 23)(17, 21)};
    
\addplot[
    color=blue!20,
    mark=square,
    ]
    coordinates {(0,81)(1, 82)(2, 25)(3, 1)(4, 3)(5, 0)(6, 18)(7, 38)(8, 27)(9, 105)(10, 90)(11, 0)(12, 14)(13, 29)(14, 37)(15, 76)(16, 24)(17, 15)};
    
\addplot[
    color=blue!15,
    mark=square,
    ]
    coordinates {(0,20)(1, 206)(2, 128)(3, 4)(4, 7)(5, 1)(6, 25)(7, 25)(8, 120)(9, 217)(10, 395)(11, 0)(12, 23)(13, 58)(14, 85)(15, 157)(16, 55)(17, 37)};

    \legend{Topic 0,Topic 1, Topic 2, Topic 3, Topic 4, Topic 5, Topic 6, Topic 7, Topic 8, Topic 9, Topic 10, Topic 11, Topic 12, Topic 13, Topic 14, Topic 15, Topic 16, Topic 17}
    
\end{axis}
\end{tikzpicture}
\caption{Alternative Darstellung der Matrix \ref{fig:matrixnormal}}
\label{fig:matrixgraph}
\end{center}
%\end{figure}

Die Erkenntnis kann auf die hohe Anzahl der Dokumente in diesen Abteilungen zurückgeführt werden. Eine alternative Erklären könnte sein, dass die Dokumente in den Abteilungen ein sehr breites semantisches Spektum abdecken, womit sich viele Topics identifizieren können. Wählte man für jede Topic die Abteilung mit der maximalen Überschneidung, bekommt man die Liste in Abbildung \ref{fig:maxlist}, die jedem Index, der die Topics repräsentiert, die Abteilungen als Werte zuordnet.

\begin{figure}[H]
\begin{center}
\begin{lstlisting}[language=Python]
maxmatrix = [2,2,9,9,9,9,10,0,10,10,1,9,9,10,9,10,9,9]
\end{lstlisting}
\caption{Liste der zugeordneten Topics und Abteilungen bei maximaler Überschneidung pro Topic}
\label{fig:maxlist}
\end{center}
\end{figure}

\textbf{HIER DIE NEUE CHART HIN MIT ÜBERSHCNIEDUNGEN IM VGL ZU BETRAG DER ABETILUNG BZW TOPIC}

Wie zu erkennen ist, sind nur die Zahlen $ 0,1,2,9,10 $ in der Liste vertreten. Eine optimale Zuordnung wäre jedoch erst erreicht, wenn alle Zahlen von 1 bis 18 ohne Duplikate in der Liste in einer beliebigen Reihenfolge vorkommen. Dies liegt daran, dass eine Topic nur genau eine Abteilung darstellen darf. In der Liste wird jedoch deutlich, dass viele Themen des LDA Modells mit dem neunten Thema der ZVO kompatibel wären. Dabei werden $9/18$ Topics Abteilung $9$, $1/18$ Topic Abteilung $1$, $5/18$ Topics Abteilung $10$ und $2/18$ Topics Abteilung $2$ zugeordnet. Hier fällt auf, dass diesen Abteilungen die meisten Dokumente von der ZVO händisch zugeordnet wurden. Somit können sie sich auch mit vielen Dokumenten aus den LDA Topics überschneiden. Dies lässt zusätzlich vermuten, dass zum Beispiel die Abteilung 9, die laut Abbildung \ref{fig:count2} die meisten Dokumente enthält, ein zu breites inhaltliches Spektrum abdeckt und in noch weitere Unterthemen unterteilt werden könnte. Dies wird dadurch begründet, dass das Topic-Modell viele der Dokumente aus Abteilung 9 in verschiedene Topics eingeteilt hat.\\

Die Zuteilung anhand der maximalen absoluten Überschneidungen ist nicht injektiv. Für die Verarbeitung der Daten ist die injektive Zuordnung jedoch fundamental wichtig, aber schwierig zu erreichen. Für die Beurteilung der Matrix führen wir einen weiteren Parameter ein, die durchschnittliche Überschneidung. Dadurch kann die Kompatibilität der Abteilungen im Bezug auf das Matching mit Topics besser analysiert werden. Vorallem bei Abteilungen, die eine sehr unausgeglichene Überschneidungsmengen haben, da sie deutlich mehr Dokumente als andere enthalten, bietet der Durchschnitt eine alternative Sichtweise. Für die durchschnittliche Überschneidung wird zuerst die Summe aller Dokumente einer Abteilung errechnet, indem alle Werte einer Spalte addiert werden. Daraufhin entsteht eine neue Matrix, in der jeder Wert jeweils durch die Summe seiner Spalte dividiert wird. Dabei wird die Anzahl an Dokumenten in jeder Abteilung irrelevant für das Endergebnis. Somit beschreibt jeder Wert, wie groß der Anteil der aus dieser Topic überschneidenden Dokumente bezogen auf die gesamte Abteilung ist in $\%$: \\

\begin{figure}[H]
\begin{center}
\tiny
\renewcommand{\arraystretch}{2}
\begin{tabular}{ccccccccccccccccccc}
\hline
\hline
&0&1&2&3&4&5&6&7&8&9&10&11&12&13&14&15&16&17\\\hline
0&5.68&4.64&15.09&8.94&5.05&7.57&5.26&6.95&9.67&4.08&4.36&4.65&7.33&6.2&3.71&6.25&6.7&9.26\\\hline
1&5.24&2.75&4.56&1.28&2.75&2.79&3.47&2.8&3.93&1.92&2.76&0&2.3&3.93&1.12&1.62&2.59&1.96\\\hline
2&5.2&3.58&1.85&3.83&5.13&8.37&6.34&4.02&3&6.37&5.84&2.33&4.15&3.12&5.88&4.35&3.73&4.81\\\hline
3&1.11&0.85&0.69&0.85&1.15&1.2&0.82&1.58&2.51&1.84&1.05&0&1.19&0.57&1.88&1.63&1.56&1\\\hline
4&5.05&4.12&3.1&2.13&4.82&3.19&3.34&4.99&4.79&4.72&5&18.6&2.55&3.62&5.48&4.03&4.02&3.3\\\hline
5&0.93&2.22&0.69&2.13&1.91&0.8&1.5&2.08&1.72&2.04&1.49&2.33&1.92&1.32&2.08&2.89&2.38&1.74\\\hline
6&12.93&18.96&12.2&11.91&9.72&19.12&20.23&16.26&14.71&17.59&22.88&13.95&19.99&21.8&19.89&17.15&14.32&18.33\\\hline
7&34.55&6.16&6.11&7.23&40.09&6.37&5.51&9.36&7.04&8.09&8.42&20.93&6.45&3.96&8.15&7.35&11.31&4.93\\\hline
8&3.17&2.78&1.52&4.26&2.07&1.99&1.67&3.16&2.16&3.28&4.9&0&2.83&3.36&3.24&3.63&4.32&2.7\\\hline
9&0.59&0.6&0.33&0&0.23&0.4&0.42&0.44&1.39&0.74&1.93&0&0.52&0.57&0.77&0.57&1.24&0.63\\\hline
10&2.93&7.64&12.17&5.11&3.9&10.76&7.08&8.28&9.12&6.66&6.44&9.3&8.34&9.29&8.28&5.55&6.4&9.19\\\hline
11&1.99&1.94&2.65&3.4&2.07&1.2&2.2&3.38&1.91&2.88&2.27&0&2.79&1.8&2.05&3.26&2.99&4.37\\\hline
12&4.97&11.18&17.51&6.81&6.12&9.56&14.07&9.06&10.68&11.35&7.32&6.98&10.85&13.04&10.88&11.36&12.19&12.11\\\hline
13&1.55&1.87&1.39&2.13&1.76&1.59&1.6&2.3&1.7&1.76&3.09&2.33&2.37&2&1.51&2.27&3.18&2.52\\\hline
14&10.32&28.89&18.82&36.6&11.71&24.3&24.69&22.71&23.62&24.35&18.53&18.6&24.46&24.2&22.84&25.51&20.91&20.44\\\hline
15&2.76&0.45&0.25&1.28&0.77&0.4&0.72&0.89&0.49&0.97&1.37&0&0.66&0.32&0.75&0.67&0.48&0.78\\\hline
16&0.83&0.39&0.18&0.43&0.23&0&0.45&1.05&0.29&0.45&0.44&0&0.49&0.3&0.45&0.63&0.51&0.56\\\hline
17&0.2&0.98&0.9&1.7&0.54&0.4&0.62&0.69&1.28&0.92&1.91&0&0.8&0.6&1.04&1.29&1.16&1.37\\\hline\hline
$\sum$&100&100&100&100&100&100&100&100&100&100&100&100&100&100&100&100&100&100\\\hline \hline
\end{tabular}
\caption{Durchschnittliche Überschneidungen aller Topics mit Labels}
\end{center}
\end{figure}

Wählt man aus jeder Zeile das maximale Element aus und notiert die assoziierten Abteilungen in einer Liste, dann ergibt sich das folgende Ergebnis. Der Unterschied bei der Darstellung ist nun, dass die Zahlen pro Zeile nicht mehr als absolutes Maß betrachtet werden, sonder als relative Häufigkeit in der jeweiligen Abteilung. Somit macht es keinen Unterschied, dass die Abteilungen verschieden viele Dokumente von der ZVO zugeordnet bekommen haben. \\

\begin{figure}[h]
\begin{center}
\begin{lstlisting}[language=Python]
average_row = [2,0,5,8,11,15,10,4,10,10,2,17,2,16,3,0,7,10]
\end{lstlisting}
\caption{Zuordnung zwischen LDA und Label} 
\end{center}
\end{figure}\\

Man erkennt bereits eine Verbesserung zu Abbildung \ref{fig:matrixnormal}, da in der Zuteilung mehr Abteilungen zugeordnet wurden. Insgesamt sind $12/18$ in der Liste vertreten. Wie bei der ersten Liste, bei der  $ 0,1,2,9,10 $ mehrfach zugeweisen wurden, sind hier $2$ und $10$ drei mal zugewiesen und $0$ zwei mal. Somit unterstreicht das Ergebnis die Problematik der Zuteilung der ZVO in Bezug auf diese Abteilungen. 

\end{enumerate}


\chapter{Zusammenfassung und Ausblick}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%ZUSAMMENFASSUNG
% In a German thesis write: \subsection{Zusammenfassung und Ausblick}

In dieser Arbeit wurde das Ziel verfolgt, die Qualität des Klassifikators zu analysieren und die Gruppierung der Abteilungen der ZVO zu untersuchen. Dabei beschäftigt sich der eine Teil der Arbeit mit der reinen Topic Modellierung und dessen Analyse. 
Bei dieser wurde die Erkenntnis gewonnen, dass die Stoppwörter noch weiter ausgebaut werden müssen, um die Qualität zu verbessern. Zu diesen Stoppwörtern gehören unter anderem die folgenden: ostholstein, sierksdorf, zweckverband, nachricht, betreff, danken, the und sitzen. 
Im zweiten Teil der Arbeit wurden die Abteilungen untersucht, indem versucht wurde, diesen die Topics zuzuordnen. Aufgefallen ist, dass die Zuteilung der Dokument sehr ungleich ist und Abteilungen wie L9 oder L10 zu viele Dokumente enthalten und in eindeutigere Unterthemen aufgeteilt werden sollten, um die Klassifikation zu verbessern. 



% !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
% !!! Your action is needed here !!!
% !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
%
% Replace the following with your conclusion



...



% Normally, the bibliography comes next at this point. Do *not* (try
% to) include further indices and tables like an index or
% a list of figures or a list of tables or such things. Nobody
% actually uses them and they just use up space. 
%
% You *can* however include a glossary, if this seems appropriate. It
% goes here as an unnumbered chapter. Most thesis will *not* need a
% glossary: a well-written text (re)explains strange words and
% concepts as necessary. However, there are situations where a
% glossary may be helpful.














%%%
% 
% Bibliographies
%
%%%
%
% The uzl-thesis class will load biblatex for the bibliography
% management. This is a powerful package, see its documentation for
% details. The styles will be setup correctly and automatically by
% choosing one of the two style keys as described earlier.
%
% In order for the bibliography to work, run latex in the following
% order (which is the standard order):
% 
% > lualatex thesis-example
% > bibtex thesis-example
% > lualatex thesis-example
% 
% Add BibTeX files using \addbibresource or use the {bibtex entries}
% environment (see below).
%
%%%
%
% Although everyting is normally setup automatically, you can change
% the options passed to biblatex using the key 'biblatex';
% for instance,
%
%   \UzLThesisSetup{biblatex={firstinits=false}}
%
% will switch off shortened first names. Normally, you will not need
% this key in your preamble. 
% 
% Note that the bibtex program is used as the 'backend' of biblatex
% by default (rather than biber, which is the preferred program of
% biblatex). This means that you can (and must) run *bibtex* after you
% have run lualatex on your thesis. If you wish to use biber instead
% of bibtex, say 'biblatex={backend=biber}'. 
% 
%%%
%
% The following environment is optional. It allows you to keep the
% bibtex entries for your thesis right here in the thesis file. What
% happens is that each time this tex file is processed, the contents
% of the following environment gets written to the file
% \jobname-bibtex-entries.bib (this file gets overwritten each
% time). Independently, \addbibresource{\jobname-bibtex-entries.bib}
% is always called if the file \jobname-bibtex-entries.bib
% exists. 
%
% In result, you can edit and keep the bibliography's bibtex entries
% right here. If you change something here, run latex, then bibtex,
% then latex once more.
%
% If you would like to manage the bibtex entries in a separate file,
% remove the below environment, delete the \jobname-bibtex-entries.bib
% file and instead write
%
% \addbibresource{filename-of-your-bibtex-file.bib}
%
% in the preamble.
%
%%%


% !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
% !!! Your action is needed here !!!
% !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
%
% Replace following example entries with the ones of your thesis.

\begin{bibtex-entries}

@Book{Knuth1986,
  author =       {Donald Erwin Knuth},
  title =        {The \TeX book},
  publisher =    {Addison-Wesley},
  year =         {1986},
}

@Book{Lamport1994,
  author =       {Leslie Lamport},
  title =        {\LaTeX: A Document Preparation System},
  publisher =    {Addison-Wesley},
  edition =      {Second edition},
  year =         {1994},
}

@TechReport{Kernighan1974,
  author =       {Brian Kernighan},
  title =        {Programming in C – A Tutorial},
  institution =  {Bell Laboratories},
  year =         {1974}
}

@Manual{Tantau2019,
  author =       {Till Tantau},
  title =        {The Ti\emph kZ and PGF Packages: Manual for version 3.1.3},
  institution =  {Institut für Theoretische Informatik, Universität zu Lübeck},
  year =         {2019},
  url =          {https://github.com/pgf-tikz/pgf}
}

@Book{Alley1996,
  author =       {Michael Alley},
  title =        {The Craft of Scientific Writing},
  publisher =    {Springer},
  year =         {1996},
  edition =      {Third Edition},
}

@Book{DowneyF13,
  author =       {R. G. Downey and M. R. Fellows},
  title =        {Fundamentals of Parameterized Complexity},
  series =       {Texts in Computer Science},
  publisher =    {Springer},
  year =         2013,
  doi =          {10.1007/978-1-4471-5559-1},
}

@Manual{biblatex,
  title =        {The \textsc{BibLaTeX} package},
  subtitle =     {Sophisticated Bibliographies in \LaTeX},
  author =       {Kime, Philip and Lehman, Philipp},
  url =          {https://github.com/plk/biblatex},
  urldate =      {2019-06-11},
  date =         {2018-10-30},
  version =      {3.12}
}

@Manual{varioref,
  title =        {The \textsc{varioref} package},
  subtitle =     {Intelligent page references},
  author =       {Mittelbach, Frank},
  url =          {http://www.ctan.org/pkg/varioref},
  urldate =      {2019-06-11},
  date =         {2016-02-16},
  version =      {1.5c}
}

@Manual{hyperref,
  title =        {The \textsc{hyperref} package},
  subtitle =     {Extensive support for hypertext in \LaTeX},
  author =       {Rahtz, Sebastian and Oberdiek, Heiko},
  url =          {https://github.com/ho-tex/hyperref},
  urldate =      {2019-06-11},
  date =         {2018-11-30},
  version =      {6.88e}
}

@Manual{babel,
  title =        {The \textsc{babel} package},
  subtitle =     {Multilingual support for Plain \TeX\ or \LaTeX},
  author =       {Braams, Johannes L. and Bezos López, Javier},
  url =          {http://www.ctan.org/pkg/babel},
  urldate =      {2019-06-11},
  date =         {2019-06-03},
  version =      {3.32}
}

@Manual{fontspec,
  title =        {The \textsc{fontspec} package},
  subtitle =     {Advanced font selection in Xe\LaTeX\ and Lua\LaTeX},
  author =       {Robertson, Will},
  url =          {http://www.ctan.org/pkg/fontspec},
  urldate =      {2019-06-11},
  version =      {2.7c}
}

@Manual{url,
  title =        {The \textsc{url} package},
  subtitle =     {Verbatim with \textsc{url}-sensitive line breaks},
  author =       {Arseneau, Donald},
  url =          {http://www.ctan.org/pkg/url},
  urldate =      {2019-06-11},
  date =         {2013-09-16},
  version =      {3.4}
}

@Manual{amsmath,
  title =        {The \textsc{amsmath} package},
  subtitle =     {\AmS\ mathematical facilities for \LaTeX},
  author =       {{The \LaTeX\ Team}},
  url =          {http://www.ams.org/tex/amslatex.html},
  urldate =      {2019-06-11}, 
  date =         {2017-09-02},
  version =      {2.17a}
}

@Book{Beutelspacher2009,
  title =        {„Das ist o.\,B.\,d.\,A.\ trivial!“: Tipps und Tricks zur
                  Formulierung mathematischer Gedanken (Mathematik für
                  Studienanfänger)},
  author =       {Albrecht Beutelspacher},
  year =         {2009},
  edition =      {Ninth, updated edition},
  publisher =    {Vieweg+Teubner Verlag},
  doi =          {10.1007/978-3-8348-9075-7},
}

\end{bibtex-entries}



% If you need to have an appendix (I advise against it), insert it
% here using, first, \appendix and then \chapter and then,
% possibly, \section. 
%
% \appendix
%
% \chapter{Technical Appendix}
%
% \section{Experimental Parameters} % possibly
%
% Again, I advise against using an appendix.


\end{document}

%  LocalWords:  LaTeX tex moretexcs Lübeck pdf uzl lualatex bibtex th
%  LocalWords:  TechReport Kernighan Lamport's Tantau's Tantau cls kZ
%  LocalWords:  Mustermann emacs oldschool pdflatex texmf utf biber
%  LocalWords:  biblatex Alphabetische Bibliographie Numerische VIIa
%  LocalWords:  varioref german Einleitung Beiträge dieser Arbeit xml
%  LocalWords:  Ergebnisse Verwandte Arbeiten Aufbau nucleotide VIIc
%  LocalWords:  ensembl amino phylogenetic Alexa Siri decrypt versa
%  LocalWords:  cryptographic pre nondeterministic deterministically
%  LocalWords:  Beutelspacher Untersuchungen zum genetischen sep llcc
%  LocalWords:  Beispiel tikz jpg png Alegrya Kasimir Malewitsch PGF
%  LocalWords:  Lamport Institut für Theoretische Informatik zu url
%  LocalWords:  Universität Springer DowneyF Downey Parameterized doi
%  LocalWords:  BibLaTeX Kime Philipp urldate Mittelbach hyperref Lua
%  LocalWords:  Rahtz Oberdiek Heiko Braams Bezos López fontspec Das
%  LocalWords:  Arseneau amsmath ist Tipps und zur Formulierung
%  LocalWords:  mathematischer Gedanken Mathematik Studienanfänger
%  LocalWords:  Albrecht Vieweg Teubner Verlag
%----------- Unbekanntes Dokument aus Abteilung 2
